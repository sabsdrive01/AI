{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae3f579",
   "metadata": {},
   "source": [
    "\n",
    "Absolutely! Below is a **Google Colab-ready notebook content** in **Markdown (MD)** format with clear explanations and **LangChain + Gemini Flash (on GCP)** code snippets for the three prompting topics you specified.\n",
    "\n",
    "You can copy this directly into a `.ipynb` cell (as Markdown for text, and Code for executable parts), or use it as a structured teaching notebook.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù **1. What is Prompting?**\n",
    "\n",
    "**Definition and Purpose**  \n",
    "Prompting is the process of giving instructions to a Large Language Model (LLM) to elicit a desired response. It acts as the primary interface between humans and LLMs.\n",
    "\n",
    "A well-crafted prompt typically includes three components:  \n",
    "- **Instruction**: What you want the model to do.  \n",
    "- **Context**: Background information to guide the response.  \n",
    "- **Task**: The specific output or action expected.\n",
    "\n",
    "> üí° **Why it matters**: On GCP with **Gemini Flash** via **LangChain**, the quality of your prompt directly impacts accuracy, cost (tokens), and reliability‚Äîespecially in production pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ **Code Snippet: Basic Prompt with LangChain + Gemini Flash**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d7d00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Selvam Sabarish\\Desktop\\sabs\\markdown_to_ipynb_converter\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Set your GCP API key (use Colab secrets for security)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m     11\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = userdata.get(\u001b[33m'\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Store your API key as 'GOOGLE_API_KEY' in Colab secrets\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Initialize Gemini Flash via LangChain\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -qU langchain-google-genai\n",
    "\n",
    "# Setup (run once per session)\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Set your GCP API key (use Colab secrets for security)\n",
    "from google.colab import userdata\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')  # Store your API key as 'GOOGLE_API_KEY' in Colab secrets\n",
    "\n",
    "# Initialize Gemini Flash via LangChain\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.3,  # Lower = more deterministic\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Simple prompt: Instruction + Context + Task\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant for a GCP cloud engineering team.\"),\n",
    "    (\"human\", \"Summarize the following paragraph in one sentence: {text}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"text\": \"Large Language Models process text in tokens. Token limits affect cost and output length. On GCP, Gemini Flash is optimized for speed and low latency.\"\n",
    "})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b11a843",
   "metadata": {},
   "source": [
    "\n",
    "‚úÖ **Tip**: Always wrap instructions in a `system` message for role/context, and user input in `human`.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù **2. Basic Prompting Techniques**\n",
    "\n",
    "#### **Zero-Shot vs. Few-Shot Prompting**\n",
    "\n",
    "- **Zero-shot**: Ask the model to perform a task **without examples**.  \n",
    "  *Example*: ‚ÄúClassify this sentence as positive or negative.‚Äù\n",
    "\n",
    "- **Few-shot**: Provide **2‚Äì3 examples** to demonstrate the pattern.  \n",
    "  *Example*:  \n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753126a9",
   "metadata": {},
   "source": [
    "\n",
    "Input: \"I love this product!\" ‚Üí Output: positive  \n",
    "  Input: \"Terrible experience.\" ‚Üí Output: negative  \n",
    "  Input: \"It's okay.\" ‚Üí Output: ?\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ecd9f1",
   "metadata": {},
   "source": [
    "\n",
    "> üí° **When to use what?**  \n",
    "> - Use **zero-shot** for simple, common tasks.  \n",
    "> - Use **few-shot** when the task is ambiguous or domain-specific (e.g., parsing logs, custom categories).\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ **Code Snippet: Few-Shot Prompting with LangChain**\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Examples for few-shot learning\n",
    "examples = [\n",
    "    {\"input\": \"The service is lightning fast!\", \"output\": \"positive\"},\n",
    "    {\"input\": \"Outage lasted 3 hours. Unacceptable.\", \"output\": \"negative\"},\n",
    "    {\"input\": \"It works, but the UI is outdated.\", \"output\": \"neutral\"}\n",
    "]\n",
    "\n",
    "# Create few-shot template\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Final prompt\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Classify user feedback about GCP services as 'positive', 'negative', or 'neutral'.\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = final_prompt | llm\n",
    "\n",
    "response = chain.invoke({\"input\": \"Gemini Flash reduced our inference cost by 40%!\"})\n",
    "print(\"Classification:\", response.content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ef8c12",
   "metadata": {},
   "source": [
    "```\n",
    "> ‚úÖ **Best Practice**: Keep examples consistent in format and domain.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù **3. Prompt Structure Best Practices**\n",
    "\n",
    "To get reliable outputs from **Gemini Flash on GCP**, structure prompts with:\n",
    "\n",
    "1. **Role Assignment**  \n",
    "   ‚Üí `\"You are a senior DevOps engineer at a GCP-focused startup.\"`  \n",
    "   Sets behavior and tone.\n",
    "\n",
    "2. **Step-by-Step Reasoning (Chain-of-Thought)**  \n",
    "   ‚Üí Ask the model to ‚Äúthink step by step‚Äù to improve reasoning on complex tasks.\n",
    "\n",
    "3. **Specify Output Format**  \n",
    "   ‚Üí Request **JSON**, **bullet points**, or **CSV** to simplify parsing in downstream systems.\n",
    "\n",
    "> üí° **Why this works**: Gemini Flash responds well to explicit formatting instructions‚Äîcritical for automating workflows in LangChain pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ **Code Snippet: Structured Output with Chain-of-Thought**\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad3c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a cloud cost optimization expert. Think step by step.\"),\n",
    "    (\"human\", \"\"\"\n",
    "    Analyze this GCP billing summary and output a JSON with:\n",
    "    - \"cost_saving_tips\": list of 2 actionable tips\n",
    "    - \"estimated_monthly_saving\": integer in USD\n",
    "\n",
    "    Respond ONLY in valid JSON. Do not include explanations.\n",
    "\n",
    "    Billing summary: \n",
    "    \"We run 10 always-on VMs (n1-standard-4) in us-central1. \n",
    "    No sustained use discounts applied. \n",
    "    Monthly spend: ~$2,800.\"\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "chain = structured_prompt | llm\n",
    "response = chain.invoke({})\n",
    "\n",
    "# Clean and parse (Gemini may add markdown JSON fences)\n",
    "import json\n",
    "cleaned = response.content.strip().removeprefix(\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4ed30",
   "metadata": {},
   "source": [
    "```json\n",
    "\").removesuffix(\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0deed8",
   "metadata": {},
   "source": [
    "```\n",
    "\").strip()\n",
    "try:\n",
    "    result = json.loads(cleaned)\n",
    "    print(json.dumps(result, indent=2))\n",
    "except json.JSONDecodeError:\n",
    "    print(\"‚ö†Ô∏è Failed to parse JSON. Raw output:\")\n",
    "    print(cleaned)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4070b2",
   "metadata": {},
   "source": [
    "```\n",
    "> ‚úÖ **Pro Tip**: Use `temperature=0` for structured outputs to reduce variability.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary for Your Learner\n",
    "\n",
    "| Concept | Key Takeaway |\n",
    "|--------|---------------|\n",
    "| **Prompting** | = Instruction + Context + Task |\n",
    "| **Zero-shot** | Fast, no examples ‚Äî good for simple tasks |\n",
    "| **Few-shot** | Better for nuanced or custom tasks |\n",
    "| **Structure** | Role + Reasoning + Format = Reliable LLM output |\n",
    "\n",
    "> üîê **Remember**: Always use **Google Colab secrets** for API keys in GCP environments‚Äînever hardcode!\n",
    "\n",
    "Let me know when you're ready for the next section (e.g., **tokens** or **hallucinations**), and I‚Äôll format it the same way!    \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
