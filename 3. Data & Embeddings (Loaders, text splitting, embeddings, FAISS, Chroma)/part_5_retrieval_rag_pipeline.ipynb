{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0f2648",
   "metadata": {},
   "source": [
    "# Part 5 â€” Retrieval & RAG Pipeline\n",
    "\n",
    "**Phase:** Retrieval-Augmented Generation (RAG) â€” from query to answer ğŸ’¡ğŸ“ŒğŸ§ ğŸ”\n",
    "\n",
    "This notebook teaches retrieval fundamentals, how to build a simple RAG pipeline using FAISS and Chroma, and hands-on demos to compare retrieval quality and performance. It's written for beginners and is Google Colab ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc0e8e",
   "metadata": {},
   "source": [
    "## Learning Guide\n",
    "\n",
    "**What you'll learn**\n",
    "\n",
    "- What retrieval (semantic search) is and why it's essential for RAG\n",
    "- Key retrieval parameters (k, score thresholds, metadata filters)\n",
    "- How to construct a simple RAG pipeline (Ingest â†’ Split â†’ Embed â†’ Store â†’ Retrieve â†’ Answer)\n",
    "- Hands-on demos using FAISS and Chroma to measure retrieval quality\n",
    "\n",
    "**Why it matters**\n",
    "\n",
    "Retrieval brings relevant context to LLMs so they can produce accurate, grounded answers without holding all knowledge in the model weights.\n",
    "\n",
    "**Hands-on steps**\n",
    "\n",
    "1. Load sample documents\n",
    "2. Chunk them into meaningful pieces\n",
    "3. Embed chunks (local or Gemini)\n",
    "4. Index chunks into FAISS and Chroma\n",
    "5. Query and compare results\n",
    "6. Send top context to an LLM (example pattern included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2495c2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'secrete_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2077946905.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msecrete_key\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmy_gemini_api_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mAPI_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_gemini_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'API_KEY loaded (hidden)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'secrete_key'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n",
    "print('API_KEY loaded (hidden)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41c7b0",
   "metadata": {},
   "source": [
    "## 5.1 What is Retrieval?\n",
    "\n",
    "- **Query â†’ embed â†’ search vectors â†’ return top-k chunks**\n",
    "\n",
    "Retrieval finds the most semantically relevant pieces of text (chunks) to use as context for an LLM. This process reduces hallucination and enables up-to-date or private-document QA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773e7e0",
   "metadata": {},
   "source": [
    "## 5.2 Key Parameters\n",
    "\n",
    "- `k` â€” number of top results to return (top-K)\n",
    "- `score_threshold` â€” minimum similarity required to accept a result\n",
    "- `metadata filters` â€” restrict search by tags, source, date, etc.\n",
    "\n",
    "Tips:\n",
    "- Start with k=3-5 for many tasks\n",
    "- Use a score threshold to avoid returning unrelated context\n",
    "- Store and query metadata to narrow search scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8954f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n",
      "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
      "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
      "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
      "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
      "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mInstall required packages if missing.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment in Colab if you need packages\n",
    "!pip install --quiet faiss-cpu sentence-transformers chromadb langchain-google-genai numpy scikit-learn\n",
    "print('Install required packages if missing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35fa9855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 5 chunks\n"
     ]
    }
   ],
   "source": [
    "# Sample documents and simple splitter\n",
    "sample_docs = [\n",
    "    \"Our office hours are 9 AM to 5 PM, Monday to Friday.\",\n",
    "    \"We offer a 30-day return policy on all products.\",\n",
    "    \"Customer support: support@company.com or call 1-800-HELP.\",\n",
    "    \"Shipping takes 3-5 business days within the US.\",\n",
    "    \"Refunds are processed within 7 business days after receiving the returned item.\",\n",
    "]\n",
    "\n",
    "import re\n",
    "\n",
    "def simple_split(doc):\n",
    "    # very small splitter (sentence-level)\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', doc) if s.strip()]\n",
    "\n",
    "# Build chunks\n",
    "chunks = []\n",
    "metadatas = []\n",
    "for i, d in enumerate(sample_docs):\n",
    "    sents = simple_split(d)\n",
    "    for j, s in enumerate(sents):\n",
    "        chunks.append(s)\n",
    "        metadatas.append({'source_doc': i})\n",
    "\n",
    "print('Prepared', len(chunks), 'chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d20b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini embeddings not available â€” name 'API_KEY' is not defined\n",
      "Using local sentence-transformers embeddings (all-MiniLM-L6-v2)\n"
     ]
    }
   ],
   "source": [
    "# Embedding function: try Gemini via LangChain, else fallback to sentence-transformers\n",
    "try:\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "    emb = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=API_KEY)\n",
    "    def get_embeddings(texts):\n",
    "        return emb.embed_documents(texts)\n",
    "    print('Using Gemini embeddings via LangChain')\n",
    "except Exception as e:\n",
    "    print('Gemini embeddings not available â€”', e)\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    def get_embeddings(texts):\n",
    "        arr = st.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        return [a.tolist() for a in arr]\n",
    "    print('Using local sentence-transformers embeddings (all-MiniLM-L6-v2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b70147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built, ntotal= 5\n"
     ]
    }
   ],
   "source": [
    "# Build FAISS index from chunks\n",
    "try:\n",
    "    import faiss\n",
    "    import numpy as np\n",
    "    vectors = get_embeddings(chunks)\n",
    "    vecs = np.array(vectors, dtype='float32')\n",
    "    dim = vecs.shape[1]\n",
    "    # Use inner product on normalized vectors to approximate cosine similarity\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(vecs)\n",
    "    print('FAISS index built, ntotal=', index.ntotal)\n",
    "except Exception as e:\n",
    "    print('Skipping FAISS build â€”', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8134a5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS query helper ready\n"
     ]
    }
   ],
   "source": [
    "# FAISS query function (returns (chunk, metadata, score))\n",
    "try:\n",
    "    import numpy as np\n",
    "    def faiss_query(query, k=3, score_threshold=None):\n",
    "        qv = np.array(get_embeddings([query]), dtype='float32')\n",
    "        D, I = index.search(qv, k)\n",
    "        results = []\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if score_threshold is not None and score < score_threshold:\n",
    "                continue\n",
    "            results.append((chunks[idx], metadatas[idx], float(score)))\n",
    "        return results\n",
    "    print('FAISS query helper ready')\n",
    "except Exception as e:\n",
    "    print('FAISS query helper not available â€”', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daf944f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma collection created, count = 5\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# New recommended way to create a persistent client\n",
    "client = chromadb.PersistentClient(path=\".chromadb_rag\")\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"rag_collection\")\n",
    "\n",
    "embeddings = get_embeddings(chunks)\n",
    "ids = [f\"chunk{i}\" for i in range(len(chunks))]\n",
    "\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=chunks,\n",
    "    metadatas=metadatas,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "\n",
    "print(\"Chroma collection created, count =\", collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3f8513d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma query helper ready\n"
     ]
    }
   ],
   "source": [
    "# Chroma query helper\n",
    "try:\n",
    "    def chroma_query(query, k=3, where=None):\n",
    "        q_emb = get_embeddings([query])[0]\n",
    "        res = collection.query(query_embeddings=[q_emb], n_results=k, where=where)\n",
    "        out = []\n",
    "        for doc, meta, score in zip(res['documents'][0], res['metadatas'][0], res['distances'][0]):\n",
    "            out.append((doc, meta, float(score)))\n",
    "        return out\n",
    "    print('Chroma query helper ready')\n",
    "except Exception as e:\n",
    "    print('Chroma query helper not available â€”', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e448bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: How do I return a product? ===\n",
      "\n",
      "FAISS results:\n",
      "- ('We offer a 30-day return policy on all products.', {'source_doc': 1}, 0.5521464943885803)\n",
      "- ('Refunds are processed within 7 business days after receiving the returned item.', {'source_doc': 4}, 0.4569006562232971)\n",
      "- ('Customer support: support@company.com or call 1-800-HELP.', {'source_doc': 2}, 0.42069706320762634)\n",
      "\n",
      "Chroma results:\n",
      "- ('We offer a 30-day return policy on all products.', {'source_doc': 1}, 0.8957072496414185)\n",
      "- ('Refunds are processed within 7 business days after receiving the returned item.', {'source_doc': 4}, 1.0861988067626953)\n",
      "- ('Customer support: support@company.com or call 1-800-HELP.', {'source_doc': 2}, 1.158605694770813)\n",
      "\n",
      "=== Query: What are the office hours? ===\n",
      "\n",
      "FAISS results:\n",
      "- ('Our office hours are 9 AM to 5 PM, Monday to Friday.', {'source_doc': 0}, 0.7283967733383179)\n",
      "- ('Shipping takes 3-5 business days within the US.', {'source_doc': 3}, 0.21840429306030273)\n",
      "- ('We offer a 30-day return policy on all products.', {'source_doc': 1}, 0.16471794247627258)\n",
      "\n",
      "Chroma results:\n",
      "- ('Our office hours are 9 AM to 5 PM, Monday to Friday.', {'source_doc': 0}, 0.5432066917419434)\n",
      "- ('Shipping takes 3-5 business days within the US.', {'source_doc': 3}, 1.5631916522979736)\n",
      "- ('We offer a 30-day return policy on all products.', {'source_doc': 1}, 1.6705641746520996)\n",
      "\n",
      "=== Query: How long for refunds? ===\n",
      "\n",
      "FAISS results:\n",
      "- ('Refunds are processed within 7 business days after receiving the returned item.', {'source_doc': 4}, 0.8311040997505188)\n",
      "- ('We offer a 30-day return policy on all products.', {'source_doc': 1}, 0.4878327548503876)\n",
      "- ('Shipping takes 3-5 business days within the US.', {'source_doc': 3}, 0.26032695174217224)\n",
      "\n",
      "Chroma results:\n",
      "- ('Refunds are processed within 7 business days after receiving the returned item.', {'source_doc': 4}, 0.3377918004989624)\n",
      "- ('We offer a 30-day return policy on all products.', {'source_doc': 1}, 1.0243346691131592)\n",
      "- ('Shipping takes 3-5 business days within the US.', {'source_doc': 3}, 1.4793461561203003)\n"
     ]
    }
   ],
   "source": [
    "# Compare FAISS vs Chroma on example queries\n",
    "queries = [\n",
    "    'How do I return a product?',\n",
    "    'What are the office hours?',\n",
    "    'How long for refunds?'\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print('\\n=== Query:', q, '===')\n",
    "    try:\n",
    "        print('\\nFAISS results:')\n",
    "        fr = faiss_query(q, k=3, score_threshold=0.1)\n",
    "        for r in fr:\n",
    "            print('-', r)\n",
    "    except Exception as e:\n",
    "        print('FAISS not available:', e)\n",
    "    try:\n",
    "        print('\\nChroma results:')\n",
    "        cr = chroma_query(q, k=3)\n",
    "        for r in cr:\n",
    "            print('-', r)\n",
    "    except Exception as e:\n",
    "        print('Chroma not available:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a9b6c",
   "metadata": {},
   "source": [
    "## 5.3 Building a Simple RAG Pipeline (Pattern)\n",
    "\n",
    "Pattern to answer a user query:\n",
    "\n",
    "1. Embed the query\n",
    "2. Retrieve top-k chunks from your vector store\n",
    "3. Concatenate top chunks into a context string (keep token budget in mind)\n",
    "4. Send a prompt to the LLM with the context and the user question, asking the LLM to answer using only the provided context\n",
    "\n",
    "Next cell shows an example using the user's Gemini invocation pattern; it falls back to printing the prompt if Gemini isn't available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28833a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending prompt to Gemini...\n",
      "\n",
      "=== Gemini response ===\n",
      "\n",
      "content=\"*   **Return Policy:** The Provider offers a 30-day return policy on all products, which Clients may utilize within that timeframe.\\n*   **Customer Support:** The Provider offers customer support via email (support@company.com) and phone (1-800-HELP) for Client inquiries.\\n*   **Availability:** The Provider's office hours for support are 9 AM to 5 PM, Monday to Friday, within which Clients should direct their calls.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--34a223d0-46ec-4b74-a9ea-419d0efeea70-0' usage_metadata={'input_tokens': 90, 'output_tokens': 1235, 'total_tokens': 1325, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1135}}\n"
     ]
    }
   ],
   "source": [
    "# Example: assemble context and call Gemini (if available)\n",
    "\n",
    "API_KEY = \"AIzaSyDN6aSfOT12MNArugxzLqGDsyskZH-AEQc\"\n",
    "\n",
    "try:\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    model = ChatGoogleGenerativeAI(model='gemini-2.5-flash', google_api_key=API_KEY)\n",
    "    query = 'Summarize Provider and Client obligations.'\n",
    "    try:\n",
    "        top = faiss_query(query, k=3)\n",
    "        context = '\\n\\n'.join([t[0] for t in top])\n",
    "    except Exception:\n",
    "        top = chroma_query(query, k=3)\n",
    "        context = '\\n\\n'.join([t[0] for t in top])\n",
    "\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a legal assistant. \n",
    "    Use ONLY the context below (do not guess).\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer concisely in 3 bullets.\"\"\"\n",
    "    \n",
    "    \n",
    "    print('Sending prompt to Gemini...')\n",
    "    resp = model.invoke(prompt)\n",
    "    print('\\n=== Gemini response ===\\n')\n",
    "    print(resp)\n",
    "except Exception as e:\n",
    "    print('Could not call Gemini â€”', e)\n",
    "    print('\\nAssembled prompt (for manual use):\\n')\n",
    "    try:\n",
    "        print(prompt)\n",
    "    except Exception:\n",
    "        print('No prompt assembled because retrieval failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7a3ea",
   "metadata": {},
   "source": [
    "## 5.4 Hands-On Demo & Exercises\n",
    "\n",
    "Exercises:\n",
    "\n",
    "1. Increase dataset size to ~100 documents and compare FAISS vs Chroma query latency.\n",
    "2. Add metadata and filter by it in Chroma queries.\n",
    "3. Tune `k` and `score_threshold` for precision vs recall.\n",
    "4. Implement a token-budget step to ensure the context fits your LLM.\n",
    "\n",
    "Use the helper functions included to iterate quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
