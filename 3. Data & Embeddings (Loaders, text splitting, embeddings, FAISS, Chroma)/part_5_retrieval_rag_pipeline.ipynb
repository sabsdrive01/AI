{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0f2648",
   "metadata": {},
   "source": [
    "# Part 5 ‚Äî Retrieval & RAG Pipeline\n",
    "\n",
    "**Phase:** Retrieval-Augmented Generation (RAG) ‚Äî from query to answer üí°üìåüß†üîç\n",
    "\n",
    "This notebook teaches retrieval fundamentals, how to build a simple RAG pipeline using FAISS and Chroma, and hands-on demos to compare retrieval quality and performance. It's written for beginners and is Google Colab ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc0e8e",
   "metadata": {},
   "source": [
    "## Learning Guide\n",
    "\n",
    "**What you'll learn**\n",
    "\n",
    "- What retrieval (semantic search) is and why it's essential for RAG\n",
    "- Key retrieval parameters (k, score thresholds, metadata filters)\n",
    "- How to construct a simple RAG pipeline (Ingest ‚Üí Split ‚Üí Embed ‚Üí Store ‚Üí Retrieve ‚Üí Answer)\n",
    "- Hands-on demos using FAISS and Chroma to measure retrieval quality\n",
    "\n",
    "**Why it matters**\n",
    "\n",
    "Retrieval brings relevant context to LLMs so they can produce accurate, grounded answers without holding all knowledge in the model weights.\n",
    "\n",
    "**Hands-on steps**\n",
    "\n",
    "1. Load sample documents\n",
    "2. Chunk them into meaningful pieces\n",
    "3. Embed chunks (local or Gemini)\n",
    "4. Index chunks into FAISS and Chroma\n",
    "5. Query and compare results\n",
    "6. Send top context to an LLM (example pattern included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2495c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n",
    "print('API_KEY loaded (hidden)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd41c7b0",
   "metadata": {},
   "source": [
    "## 5.1 What is Retrieval?\n",
    "\n",
    "- **Query ‚Üí embed ‚Üí search vectors ‚Üí return top-k chunks**\n",
    "\n",
    "Retrieval finds the most semantically relevant pieces of text (chunks) to use as context for an LLM. This process reduces hallucination and enables up-to-date or private-document QA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773e7e0",
   "metadata": {},
   "source": [
    "## 5.2 Key Parameters\n",
    "\n",
    "- `k` ‚Äî number of top results to return (top-K)\n",
    "- `score_threshold` ‚Äî minimum similarity required to accept a result\n",
    "- `metadata filters` ‚Äî restrict search by tags, source, date, etc.\n",
    "\n",
    "Tips:\n",
    "- Start with k=3-5 for many tasks\n",
    "- Use a score threshold to avoid returning unrelated context\n",
    "- Store and query metadata to narrow search scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment in Colab if you need packages\n",
    "# !pip install --quiet faiss-cpu sentence-transformers chromadb langchain-google-genai numpy scikit-learn\n",
    "print('Install required packages if missing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents and simple splitter\n",
    "sample_docs = [\n",
    "    \"Our office hours are 9 AM to 5 PM, Monday to Friday.\",\n",
    "    \"We offer a 30-day return policy on all products.\",\n",
    "    \"Customer support: support@company.com or call 1-800-HELP.\",\n",
    "    \"Shipping takes 3-5 business days within the US.\",\n",
    "    \"Refunds are processed within 7 business days after receiving the returned item.\",\n",
    "]\n",
    "\n",
    "import re\n",
    "\n",
    "def simple_split(doc):\n",
    "    # very small splitter (sentence-level)\n",
    "    return [s.strip() for s in re.split(r'(?<=[.!?])\\s+', doc) if s.strip()]\n",
    "\n",
    "# Build chunks\n",
    "chunks = []\n",
    "metadatas = []\n",
    "for i, d in enumerate(sample_docs):\n",
    "    sents = simple_split(d)\n",
    "    for j, s in enumerate(sents):\n",
    "        chunks.append(s)\n",
    "        metadatas.append({'source_doc': i})\n",
    "\n",
    "print('Prepared', len(chunks), 'chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding function: try Gemini via LangChain, else fallback to sentence-transformers\n",
    "try:\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "    emb = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=API_KEY)\n",
    "    def get_embeddings(texts):\n",
    "        return emb.embed_documents(texts)\n",
    "    print('Using Gemini embeddings via LangChain')\n",
    "except Exception as e:\n",
    "    print('Gemini embeddings not available ‚Äî', e)\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    def get_embeddings(texts):\n",
    "        arr = st.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        return [a.tolist() for a in arr]\n",
    "    print('Using local sentence-transformers embeddings (all-MiniLM-L6-v2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b70147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index from chunks\n",
    "try:\n",
    "    import faiss\n",
    "    import numpy as np\n",
    "    vectors = get_embeddings(chunks)\n",
    "    vecs = np.array(vectors, dtype='float32')\n",
    "    dim = vecs.shape[1]\n",
    "    # Use inner product on normalized vectors to approximate cosine similarity\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(vecs)\n",
    "    print('FAISS index built, ntotal=', index.ntotal)\n",
    "except Exception as e:\n",
    "    print('Skipping FAISS build ‚Äî', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS query function (returns (chunk, metadata, score))\n",
    "try:\n",
    "    import numpy as np\n",
    "    def faiss_query(query, k=3, score_threshold=None):\n",
    "        qv = np.array(get_embeddings([query]), dtype='float32')\n",
    "        D, I = index.search(qv, k)\n",
    "        results = []\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if score_threshold is not None and score < score_threshold:\n",
    "                continue\n",
    "            results.append((chunks[idx], metadatas[idx], float(score)))\n",
    "        return results\n",
    "    print('FAISS query helper ready')\n",
    "except Exception as e:\n",
    "    print('FAISS query helper not available ‚Äî', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf944f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Chroma collection\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    client = chromadb.Client(Settings(chroma_db_impl='duckdb+parquet', persist_directory='.chromadb_rag'))\n",
    "    collection = client.get_or_create_collection(name='rag_collection')\n",
    "    embeddings = get_embeddings(chunks)\n",
    "    ids = [f'chunk{i}' for i in range(len(chunks))]\n",
    "    collection.add(ids=ids, documents=chunks, metadatas=metadatas, embeddings=embeddings)\n",
    "    print('Chroma collection created, count =', collection.count())\n",
    "except Exception as e:\n",
    "    print('Skipping Chroma build ‚Äî', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f8513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma query helper\n",
    "try:\n",
    "    def chroma_query(query, k=3, where=None):\n",
    "        q_emb = get_embeddings([query])[0]\n",
    "        res = collection.query(query_embeddings=[q_emb], n_results=k, where=where)\n",
    "        out = []\n",
    "        for doc, meta, score in zip(res['documents'][0], res['metadatas'][0], res['distances'][0]):\n",
    "            out.append((doc, meta, float(score)))\n",
    "        return out\n",
    "    print('Chroma query helper ready')\n",
    "except Exception as e:\n",
    "    print('Chroma query helper not available ‚Äî', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e448bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare FAISS vs Chroma on example queries\n",
    "queries = [\n",
    "    'How do I return a product?',\n",
    "    'What are the office hours?',\n",
    "    'How long for refunds?'\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print('\\n=== Query:', q, '===')\n",
    "    try:\n",
    "        print('\\nFAISS results:')\n",
    "        fr = faiss_query(q, k=3, score_threshold=0.1)\n",
    "        for r in fr:\n",
    "            print('-', r)\n",
    "    except Exception as e:\n",
    "        print('FAISS not available:', e)\n",
    "    try:\n",
    "        print('\\nChroma results:')\n",
    "        cr = chroma_query(q, k=3)\n",
    "        for r in cr:\n",
    "            print('-', r)\n",
    "    except Exception as e:\n",
    "        print('Chroma not available:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a9b6c",
   "metadata": {},
   "source": [
    "## 5.3 Building a Simple RAG Pipeline (Pattern)\n",
    "\n",
    "Pattern to answer a user query:\n",
    "\n",
    "1. Embed the query\n",
    "2. Retrieve top-k chunks from your vector store\n",
    "3. Concatenate top chunks into a context string (keep token budget in mind)\n",
    "4. Send a prompt to the LLM with the context and the user question, asking the LLM to answer using only the provided context\n",
    "\n",
    "Next cell shows an example using the user's Gemini invocation pattern; it falls back to printing the prompt if Gemini isn't available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: assemble context and call Gemini (if available)\n",
    "try:\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    model = ChatGoogleGenerativeAI(model='gemini-2.5-flash', google_api_key=API_KEY)\n",
    "    query = 'Summarize Provider and Client obligations.'\n",
    "    try:\n",
    "        top = faiss_query(query, k=3)\n",
    "        context = '\\n\\n'.join([t[0] for t in top])\n",
    "    except Exception:\n",
    "        top = chroma_query(query, k=3)\n",
    "        context = '\\n\\n'.join([t[0] for t in top])\n",
    "    prompt = f\"\"\"You are a legal assistant. Use ONLY the context below (do not guess).\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer concisely in 3 bullets.\"\"\"\n",
    "    print('Sending prompt to Gemini...')\n",
    "    resp = model.invoke(prompt)\n",
    "    print('\\n=== Gemini response ===\\n')\n",
    "    print(resp)\n",
    "except Exception as e:\n",
    "    print('Could not call Gemini ‚Äî', e)\n",
    "    print('\\nAssembled prompt (for manual use):\\n')\n",
    "    try:\n",
    "        print(prompt)\n",
    "    except Exception:\n",
    "        print('No prompt assembled because retrieval failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d7a3ea",
   "metadata": {},
   "source": [
    "## 5.4 Hands-On Demo & Exercises\n",
    "\n",
    "Exercises:\n",
    "\n",
    "1. Increase dataset size to ~100 documents and compare FAISS vs Chroma query latency.\n",
    "2. Add metadata and filter by it in Chroma queries.\n",
    "3. Tune `k` and `score_threshold` for precision vs recall.\n",
    "4. Implement a token-budget step to ensure the context fits your LLM.\n",
    "\n",
    "Use the helper functions included to iterate quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
