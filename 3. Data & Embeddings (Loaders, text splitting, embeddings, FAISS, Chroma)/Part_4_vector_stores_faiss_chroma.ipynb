{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "99fb1f48",
      "metadata": {
        "id": "99fb1f48"
      },
      "source": [
        "# Vector Stores (FAISS & Chroma) â€” Practical Workshop\n",
        "\n",
        "**Phase:** Storing embeddings for fast semantic search ğŸ’¡ğŸ“ŒğŸ§ ğŸ”\n",
        "\n",
        "This notebook teaches vector stores (FAISS and Chroma), how they work, and how to build a RAG index. It includes hands-on demos you can run in Google Colab or locally."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1abb7fc3",
      "metadata": {
        "id": "1abb7fc3"
      },
      "source": [
        "## Learning Guide\n",
        "\n",
        "**What you will learn**\n",
        "\n",
        "- What vector stores are and why they matter for semantic search and RAG\n",
        "- Differences between FAISS and ChromaDB (strengths & trade-offs)\n",
        "- How to build, save, and query FAISS and Chroma indexes\n",
        "- How to assemble a simple RAG pipeline: Ingest â†’ Split â†’ Embed â†’ Store â†’ Query\n",
        "\n",
        "**Why it matters**\n",
        "\n",
        "Vector stores let you search millions of embeddings quickly and are the backbone of retrieval-augmented generation systems.\n",
        "\n",
        "**Hands-on steps**\n",
        "\n",
        "1. Install dependencies (optional cell provided)\n",
        "2. Load or create sample documents\n",
        "3. Chunk documents (we provide a splitter example)\n",
        "4. Generate embeddings (local or Gemini)\n",
        "5. Build FAISS and Chroma stores and run queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e76d8f2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e76d8f2a",
        "outputId": "3f90e7bd-dd67-47d2-8bc8-255100284d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API_KEY loaded (hidden)\n"
          ]
        }
      ],
      "source": [
        "from secrete_key import my_gemini_api_key\n",
        "API_KEY = my_gemini_api_key()\n",
        "print('API_KEY loaded (hidden)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "39279c44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39279c44",
        "outputId": "8cb0b048-b2b7-44b8-a471-7a2f578527b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.4/132.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m181.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.0 which is incompatible.\n",
            "google-adk 1.19.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.0 which is incompatible.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mIf packages are missing, run the pip install line above in your environment.\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to install dependencies in Colab or a fresh VM\n",
        "# !pip install --quiet faiss-cpu sentence-transformers chromadb langchain-google-genai numpy scikit-learn\n",
        "print('If packages are missing, run the pip install line above in your environment.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ebcb17a",
      "metadata": {
        "id": "0ebcb17a"
      },
      "source": [
        "## 4.1 What Are Vector Stores?\n",
        "\n",
        "Vector stores (a.k.a. vector databases or indexes) store embeddings and support fast nearest-neighbor search. Key concepts:\n",
        "\n",
        "- **Indexing**: Organizes vectors to make search fast (flat, IVF, HNSW, etc.)\n",
        "- **Distance metric**: L2 (Euclidean) or cosine are common\n",
        "- **Metadata**: Many stores support storing document metadata for filtered search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a36a661",
      "metadata": {
        "id": "9a36a661"
      },
      "source": [
        "## 4.2 FAISS\n",
        "\n",
        "**Facebook AI Similarity Search** â€” a high-performance library for vector similarity search.\n",
        "\n",
        "- Strengths: Extremely fast, handles millions of vectors, many index types (Flat, IVFFlat, HNSW, PQ)\n",
        "- Weaknesses: Lower-level API, you must manage metadata externally, persistence is via file saves\n",
        "\n",
        "We'll build a simple FAISS index (IndexFlatL2) for demo purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d239f57b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d239f57b",
        "outputId": "4d357673-9df7-41c0-f72e-6de1af5becff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS demo will run if `faiss` and `sentence_transformers` are installed.\n",
            "Built FAISS index with dimension 384 and 5 vectors\n",
            "What is programming?\n",
            "\n",
            "Search results:\n",
            "1. Python is a programming language (index 0, distance 0.7474)\n",
            "2. Java is also a programming language (index 1, distance 0.7558)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print('FAISS demo will run if `faiss` and `sentence_transformers` are installed.')\n",
        "\n",
        "sample_documents = [\n",
        "    'Python is a programming language',\n",
        "    'Java is also a programming language',\n",
        "    'The weather is sunny today',\n",
        "    'Machine learning uses algorithms',\n",
        "    'Deep learning is a subset of machine learning',\n",
        "]\n",
        "\n",
        "try:\n",
        "    import faiss\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = model.encode(sample_documents, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    print(f'Built FAISS index with dimension {dim} and {index.ntotal} vectors')\n",
        "    # Query example\n",
        "    query = 'What is programming?'\n",
        "    print(query)\n",
        "    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
        "    k = 2\n",
        "    distances, indices = index.search(q_emb, k)\n",
        "    print('\\nSearch results:')\n",
        "    for rank, idx in enumerate(indices[0]):\n",
        "        print(f\"{rank+1}. {sample_documents[idx]} (index {idx}, distance {distances[0][rank]:.4f})\")\n",
        "except Exception as e:\n",
        "    print('Skipping FAISS demo â€” ensure faiss-cpu and sentence-transformers are installed. Error:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f3eb2f71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3eb2f71",
        "outputId": "4cc4ef3d-eff1-4c5f-e706-fb893548aa01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved FAISS index to faiss_demo.index\n",
            "Loaded FAISS index, ntotal = 5\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import faiss\n",
        "    # Save index\n",
        "    faiss.write_index(index, 'faiss_demo.index')\n",
        "    print('Saved FAISS index to faiss_demo.index')\n",
        "    # Load index\n",
        "    loaded = faiss.read_index('faiss_demo.index')\n",
        "    print('Loaded FAISS index, ntotal =', loaded.ntotal)\n",
        "except Exception as e:\n",
        "    print('Could not save/load FAISS index â€”', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c657e389",
      "metadata": {
        "id": "c657e389"
      },
      "source": [
        "## 4.3 ChromaDB\n",
        "\n",
        "Chroma is a simple, developer-friendly vector store with built-in persistence and metadata support. It's great for prototypes and small-to-medium projects.\n",
        "\n",
        "- Strengths: Easy to use, metadata filtering, persistent by default\n",
        "- Weaknesses: Not yet as optimized as FAISS for very large datasets\n",
        "\n",
        "We'll demonstrate creating a Chroma collection, inserting documents, and querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b9444fc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9444fc4",
        "outputId": "1b007af9-5216-4f75-8fd4-57c950ec93f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chroma import OK\n",
            "Collection 'demo_collection' created.\n",
            "Added 4 new documents to Chroma collection.\n",
            "Total documents in collection: 4\n",
            "\n",
            "Chroma query results:\n",
            "- Python is great for data science\n",
            "- SQL is used for databases\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import chromadb\n",
        "    from chromadb.config import Settings\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    print('Chroma import OK')\n",
        "    # Initialize client (persistent)\n",
        "    # Updated client initialization as per ChromaDB recommendations\n",
        "    client = chromadb.PersistentClient(path=\".chromadb\")\n",
        "\n",
        "    # Get or create collection\n",
        "    collection_name = 'demo_collection'\n",
        "    try:\n",
        "        collection = client.get_collection(name=collection_name)\n",
        "        print(f\"Collection '{collection_name}' already exists.\")\n",
        "    except:\n",
        "        collection = client.create_collection(name=collection_name)\n",
        "        print(f\"Collection '{collection_name}' created.\")\n",
        "\n",
        "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    docs = [\n",
        "        'Python is great for data science',\n",
        "        'JavaScript runs in browsers',\n",
        "        'SQL is used for databases',\n",
        "        'Machine learning predicts outcomes',\n",
        "    ]\n",
        "    ids = [f'doc{i}' for i in range(len(docs))]\n",
        "    embeddings = st.encode(docs, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    # Add to collection (Chroma can accept precomputed embeddings)\n",
        "    # Check if documents already exist before adding to avoid duplicates in persistent client\n",
        "    existing_ids = collection.get(ids=ids, include=[])['ids']\n",
        "    docs_to_add = []\n",
        "    ids_to_add = []\n",
        "    embeddings_to_add = []\n",
        "    for i, doc_id in enumerate(ids):\n",
        "        if doc_id not in existing_ids:\n",
        "            docs_to_add.append(docs[i])\n",
        "            ids_to_add.append(ids[i])\n",
        "            embeddings_to_add.append(embeddings[i].tolist())\n",
        "\n",
        "    if docs_to_add:\n",
        "        collection.add(documents=docs_to_add, ids=ids_to_add, embeddings=embeddings_to_add)\n",
        "        print(f'Added {len(docs_to_add)} new documents to Chroma collection.')\n",
        "    else:\n",
        "        print('No new documents to add, they already exist.')\n",
        "\n",
        "    print('Total documents in collection:', collection.count())\n",
        "    # Query\n",
        "    query = 'Tell me about Python'\n",
        "    q_emb = st.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()\n",
        "    results = collection.query(query_embeddings=[q_emb], n_results=2)\n",
        "    print('\\nChroma query results:')\n",
        "    for doc in results['documents'][0]:\n",
        "        print('-', doc)\n",
        "except Exception as e:\n",
        "    print('Skipping Chroma demo â€” ensure chromadb and sentence-transformers are installed. Error:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fabfc194",
      "metadata": {
        "id": "fabfc194"
      },
      "source": [
        "## 4.4 Building a RAG Index (Ingest â†’ Split â†’ Embed â†’ Store)\n",
        "\n",
        "High-level steps:\n",
        "\n",
        "1. **Ingest** documents from files or a database\n",
        "2. **Split** into chunks using an appropriate splitter (e.g., RecursiveCharacterTextSplitter or semantic chunker)\n",
        "3. **Embed** chunks using an embeddings provider (Gemini or sentence-transformers for offline)\n",
        "4. **Store** embeddings + metadata in a vector store (FAISS, Chroma)\n",
        "5. **Query**: embed the user query, search the store, and pass top chunks as context to the LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0ae3d6e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ae3d6e2",
        "outputId": "06e1a53a-538f-4902-f7cd-8159a5d314d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built FAISS index for RAG with 4 chunks\n",
            "\n",
            "RAG demo query: \"How do I return a product?\"\n",
            "\n",
            "Chunk: We offer a 30-day return policy on all products.\n",
            "Metadata: {'source_doc': 1}\n",
            "Distance: 0.8957071304321289\n",
            "\n",
            "Chunk: Customer support: support@company.com\n",
            "Metadata: {'source_doc': 2}\n",
            "Distance: 1.1979711055755615\n"
          ]
        }
      ],
      "source": [
        "# Example RAG pipeline (small demo)\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import faiss\n",
        "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    docs = [\n",
        "        'Our office hours are 9 AM to 5 PM, Monday to Friday.',\n",
        "        'We offer a 30-day return policy on all products.',\n",
        "        'Customer support: support@company.com',\n",
        "        'Shipping takes 3-5 business days within the US.'\n",
        "    ]\n",
        "    # Simple splitter: split into sentences (for demo)\n",
        "    import re\n",
        "    def simple_split(doc):\n",
        "        return [s.strip() for s in re.split(r'(?<=[.!?])\\\\s+', doc) if s.strip()]\n",
        "    chunks = []\n",
        "    metadatas = []\n",
        "    for i, d in enumerate(docs):\n",
        "        sents = simple_split(d)\n",
        "        for j, s in enumerate(sents):\n",
        "            chunks.append(s)\n",
        "            metadatas.append({'source_doc': i})\n",
        "    embeddings = st.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    print('Built FAISS index for RAG with', index.ntotal, 'chunks')\n",
        "    # Query pipeline\n",
        "    def rag_query(query, k=2):\n",
        "        q_emb = st.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
        "        distances, indices = index.search(q_emb, k)\n",
        "        return [(chunks[idx], metadatas[idx], float(distances[0][i])) for i, idx in enumerate(indices[0])]\n",
        "    print('\\nRAG demo query: \"How do I return a product?\"')\n",
        "    results = rag_query('How do I return a product?', k=2)\n",
        "    for r in results:\n",
        "        print('\\nChunk:', r[0])\n",
        "        print('Metadata:', r[1])\n",
        "        print('Distance:', r[2])\n",
        "except Exception as e:\n",
        "    print('Skipping RAG pipeline demo â€” ensure sentence-transformers and faiss are installed. Error:', e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3e0ba0a",
      "metadata": {
        "id": "c3e0ba0a"
      },
      "source": [
        "## 4.5 Hands-On Demo & FAISS vs Chroma Comparison\n",
        "\n",
        "Try both FAISS and Chroma on your dataset and measure:\n",
        "\n",
        "- Build time\n",
        "- Query latency\n",
        "- Ease of working with metadata\n",
        "- Persistence and portability\n",
        "\n",
        "Guidance:\n",
        "\n",
        "- Use FAISS if you need raw speed and are comfortable managing metadata and persistence yourself.\n",
        "- Use Chroma for rapid prototyping, metadata filtering, and persistence out of the box.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5a1ee123",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a1ee123",
        "outputId": "736a06e7-e31c-45b9-a462-f8f04a223099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS add time: 0.002s, search time: 0.000270s\n",
            "Chroma perf test skipped â€” chromadb not available or error: Collection [perf_test] already exists\n"
          ]
        }
      ],
      "source": [
        "# Performance test stub: generate N random vectors and time FAISS vs simple Chroma ops\n",
        "try:\n",
        "    import time\n",
        "    import numpy as np\n",
        "    N = 1000\n",
        "    dim = 384\n",
        "    vectors = np.random.random((N, dim)).astype('float32')\n",
        "    # FAISS timing\n",
        "    import faiss\n",
        "    idx = faiss.IndexFlatL2(dim)\n",
        "    t0 = time.time()\n",
        "    idx.add(vectors)\n",
        "    t1 = time.time()\n",
        "    # search\n",
        "    q = np.random.random((1, dim)).astype('float32')\n",
        "    t2 = time.time()\n",
        "    d, ix = idx.search(q, 5)\n",
        "    t3 = time.time()\n",
        "    print(f'FAISS add time: {t1-t0:.3f}s, search time: {t3-t2:.6f}s')\n",
        "    # Chroma timing (approx)\n",
        "    try:\n",
        "        import chromadb\n",
        "        from chromadb.config import Settings\n",
        "        client = chromadb.PersistentClient(path='.chromadb_perf') # Updated to PersistentClient\n",
        "        coll = client.create_collection('perf_test')\n",
        "        coll.add(ids=[str(i) for i in range(N)], documents=['doc']*N, embeddings=vectors.tolist())\n",
        "        t4 = time.time()\n",
        "        res = coll.query(query_embeddings=[q.flatten().tolist()], n_results=5)\n",
        "        t5 = time.time()\n",
        "        print(f'Chroma add+query approx time: {(t5-t4):.3f}s (query part)')\n",
        "    except Exception as e:\n",
        "        print('Chroma perf test skipped â€” chromadb not available or error:', e)\n",
        "except Exception as e:\n",
        "    print('Performance test skipped â€”', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abad92cd",
      "metadata": {
        "id": "abad92cd"
      },
      "source": [
        "## Best practices\n",
        "\n",
        "- **Choose chunk size** to preserve semantic units â€” for contracts, larger chunks (500-1000 chars) often work better.\n",
        "- **Overlap** helps keep context between chunks: 50-200 characters or token-equivalent.\n",
        "- **Normalize embeddings** if you use cosine similarity with L2 indexes.\n",
        "- **Persist** FAISS indexes to disk and store metadata in a separate table (e.g., SQLite) if using FAISS in production.\n",
        "- **Test** retrieval quality with real queries before bulk ingestion.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
