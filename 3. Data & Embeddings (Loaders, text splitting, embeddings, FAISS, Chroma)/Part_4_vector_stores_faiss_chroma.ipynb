{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fb1f48",
   "metadata": {},
   "source": [
    "# Vector Stores (FAISS & Chroma) ‚Äî Practical Workshop\n",
    "\n",
    "**Phase:** Storing embeddings for fast semantic search üí°üìåüß†üîç\n",
    "\n",
    "This notebook teaches vector stores (FAISS and Chroma), how they work, and how to build a RAG index. It includes hands-on demos you can run in Google Colab or locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abb7fc3",
   "metadata": {},
   "source": [
    "## Learning Guide\n",
    "\n",
    "**What you will learn**\n",
    "\n",
    "- What vector stores are and why they matter for semantic search and RAG\n",
    "- Differences between FAISS and ChromaDB (strengths & trade-offs)\n",
    "- How to build, save, and query FAISS and Chroma indexes\n",
    "- How to assemble a simple RAG pipeline: Ingest ‚Üí Split ‚Üí Embed ‚Üí Store ‚Üí Query\n",
    "\n",
    "**Why it matters**\n",
    "\n",
    "Vector stores let you search millions of embeddings quickly and are the backbone of retrieval-augmented generation systems.\n",
    "\n",
    "**Hands-on steps**\n",
    "\n",
    "1. Install dependencies (optional cell provided)\n",
    "2. Load or create sample documents\n",
    "3. Chunk documents (we provide a splitter example)\n",
    "4. Generate embeddings (local or Gemini)\n",
    "5. Build FAISS and Chroma stores and run queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n",
    "print('API_KEY loaded (hidden)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39279c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies in Colab or a fresh VM\n",
    "# !pip install --quiet faiss-cpu sentence-transformers chromadb langchain-google-genai numpy scikit-learn\n",
    "print('If packages are missing, run the pip install line above in your environment.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcb17a",
   "metadata": {},
   "source": [
    "## 4.1 What Are Vector Stores?\n",
    "\n",
    "Vector stores (a.k.a. vector databases or indexes) store embeddings and support fast nearest-neighbor search. Key concepts:\n",
    "\n",
    "- **Indexing**: Organizes vectors to make search fast (flat, IVF, HNSW, etc.)\n",
    "- **Distance metric**: L2 (Euclidean) or cosine are common\n",
    "- **Metadata**: Many stores support storing document metadata for filtered search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a36a661",
   "metadata": {},
   "source": [
    "## 4.2 FAISS\n",
    "\n",
    "**Facebook AI Similarity Search** ‚Äî a high-performance library for vector similarity search.\n",
    "\n",
    "- Strengths: Extremely fast, handles millions of vectors, many index types (Flat, IVFFlat, HNSW, PQ)\n",
    "- Weaknesses: Lower-level API, you must manage metadata externally, persistence is via file saves\n",
    "\n",
    "We'll build a simple FAISS index (IndexFlatL2) for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print('FAISS demo will run if `faiss` and `sentence_transformers` are installed.')\n",
    "\n",
    "sample_documents = [\n",
    "    'Python is a programming language',\n",
    "    'Java is also a programming language',\n",
    "    'The weather is sunny today',\n",
    "    'Machine learning uses algorithms',\n",
    "    'Deep learning is a subset of machine learning',\n",
    "]\n",
    "\n",
    "try:\n",
    "    import faiss\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(sample_documents, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    print(f'Built FAISS index with dimension {dim} and {index.ntotal} vectors')\n",
    "    # Query example\n",
    "    query = 'What is programming?'\n",
    "    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
    "    k = 2\n",
    "    distances, indices = index.search(q_emb, k)\n",
    "    print('\\nSearch results:')\n",
    "    for rank, idx in enumerate(indices[0]):\n",
    "        print(f\"{rank+1}. {sample_documents[idx]} (index {idx}, distance {distances[0][rank]:.4f})\")\n",
    "except Exception as e:\n",
    "    print('Skipping FAISS demo ‚Äî ensure faiss-cpu and sentence-transformers are installed. Error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import faiss\n",
    "    # Save index\n",
    "    faiss.write_index(index, 'faiss_demo.index')\n",
    "    print('Saved FAISS index to faiss_demo.index')\n",
    "    # Load index\n",
    "    loaded = faiss.read_index('faiss_demo.index')\n",
    "    print('Loaded FAISS index, ntotal =', loaded.ntotal)\n",
    "except Exception as e:\n",
    "    print('Could not save/load FAISS index ‚Äî', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657e389",
   "metadata": {},
   "source": [
    "## 4.3 ChromaDB\n",
    "\n",
    "Chroma is a simple, developer-friendly vector store with built-in persistence and metadata support. It's great for prototypes and small-to-medium projects.\n",
    "\n",
    "- Strengths: Easy to use, metadata filtering, persistent by default\n",
    "- Weaknesses: Not yet as optimized as FAISS for very large datasets\n",
    "\n",
    "We'll demonstrate creating a Chroma collection, inserting documents, and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9444fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print('Chroma import OK')\n",
    "    # Initialize client (in-memory or persistent)\n",
    "    client = chromadb.Client(Settings(chroma_db_impl='duckdb+parquet', persist_directory='.chromadb'))\n",
    "    collection = client.create_collection('demo_collection')\n",
    "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    docs = [\n",
    "        'Python is great for data science',\n",
    "        'JavaScript runs in browsers',\n",
    "        'SQL is used for databases',\n",
    "        'Machine learning predicts outcomes',\n",
    "    ]\n",
    "    ids = [f'doc{i}' for i in range(len(docs))]\n",
    "    embeddings = st.encode(docs, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    # Add to collection (Chroma can accept precomputed embeddings)\n",
    "    collection.add(documents=docs, ids=ids, embeddings=embeddings.tolist())\n",
    "    print('Added documents to Chroma collection. Count:', collection.count())\n",
    "    # Query\n",
    "    query = 'Tell me about Python'\n",
    "    q_emb = st.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()\n",
    "    results = collection.query(query_embeddings=[q_emb], n_results=2)\n",
    "    print('\\nChroma query results:')\n",
    "    for doc in results['documents'][0]:\n",
    "        print('-', doc)\n",
    "except Exception as e:\n",
    "    print('Skipping Chroma demo ‚Äî ensure chromadb and sentence-transformers are installed. Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabfc194",
   "metadata": {},
   "source": [
    "## 4.4 Building a RAG Index (Ingest ‚Üí Split ‚Üí Embed ‚Üí Store)\n",
    "\n",
    "High-level steps:\n",
    "\n",
    "1. **Ingest** documents from files or a database\n",
    "2. **Split** into chunks using an appropriate splitter (e.g., RecursiveCharacterTextSplitter or semantic chunker)\n",
    "3. **Embed** chunks using an embeddings provider (Gemini or sentence-transformers for offline)\n",
    "4. **Store** embeddings + metadata in a vector store (FAISS, Chroma)\n",
    "5. **Query**: embed the user query, search the store, and pass top chunks as context to the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example RAG pipeline (small demo)\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import faiss\n",
    "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    docs = [\n",
    "        'Our office hours are 9 AM to 5 PM, Monday to Friday.',\n",
    "        'We offer a 30-day return policy on all products.',\n",
    "        'Customer support: support@company.com',\n",
    "        'Shipping takes 3-5 business days within the US.'\n",
    "    ]\n",
    "    # Simple splitter: split into sentences (for demo)\n",
    "    import re\n",
    "    def simple_split(doc):\n",
    "        return [s.strip() for s in re.split(r'(?<=[.!?])\\\\s+', doc) if s.strip()]\n",
    "    chunks = []\n",
    "    metadatas = []\n",
    "    for i, d in enumerate(docs):\n",
    "        sents = simple_split(d)\n",
    "        for j, s in enumerate(sents):\n",
    "            chunks.append(s)\n",
    "            metadatas.append({'source_doc': i})\n",
    "    embeddings = st.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "    print('Built FAISS index for RAG with', index.ntotal, 'chunks')\n",
    "    # Query pipeline\n",
    "    def rag_query(query, k=2):\n",
    "        q_emb = st.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype('float32')\n",
    "        distances, indices = index.search(q_emb, k)\n",
    "        return [(chunks[idx], metadatas[idx], float(distances[0][i])) for i, idx in enumerate(indices[0])]\n",
    "    print('\\nRAG demo query: \"How do I return a product?\"')\n",
    "    results = rag_query('How do I return a product?', k=2)\n",
    "    for r in results:\n",
    "        print('\\nChunk:', r[0])\n",
    "        print('Metadata:', r[1])\n",
    "        print('Distance:', r[2])\n",
    "except Exception as e:\n",
    "    print('Skipping RAG pipeline demo ‚Äî ensure sentence-transformers and faiss are installed. Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0ba0a",
   "metadata": {},
   "source": [
    "## 4.5 Hands-On Demo & FAISS vs Chroma Comparison\n",
    "\n",
    "Try both FAISS and Chroma on your dataset and measure:\n",
    "\n",
    "- Build time\n",
    "- Query latency\n",
    "- Ease of working with metadata\n",
    "- Persistence and portability\n",
    "\n",
    "Guidance:\n",
    "\n",
    "- Use FAISS if you need raw speed and are comfortable managing metadata and persistence yourself.\n",
    "- Use Chroma for rapid prototyping, metadata filtering, and persistence out of the box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ee123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance test stub: generate N random vectors and time FAISS vs simple Chroma ops\n",
    "try:\n",
    "    import time\n",
    "    import numpy as np\n",
    "    N = 1000\n",
    "    dim = 384\n",
    "    vectors = np.random.random((N, dim)).astype('float32')\n",
    "    # FAISS timing\n",
    "    import faiss\n",
    "    idx = faiss.IndexFlatL2(dim)\n",
    "    t0 = time.time()\n",
    "    idx.add(vectors)\n",
    "    t1 = time.time()\n",
    "    # search\n",
    "    q = np.random.random((1, dim)).astype('float32')\n",
    "    t2 = time.time()\n",
    "    d, ix = idx.search(q, 5)\n",
    "    t3 = time.time()\n",
    "    print(f'FAISS add time: {t1-t0:.3f}s, search time: {t3-t2:.6f}s')\n",
    "    # Chroma timing (approx)\n",
    "    try:\n",
    "        import chromadb\n",
    "        from chromadb.config import Settings\n",
    "        client = chromadb.Client(Settings(chroma_db_impl='duckdb+parquet', persist_directory='.chromadb_perf'))\n",
    "        coll = client.create_collection('perf_test')\n",
    "        coll.add(ids=[str(i) for i in range(N)], documents=['doc']*N, embeddings=vectors.tolist())\n",
    "        t4 = time.time()\n",
    "        res = coll.query(query_embeddings=[q.flatten().tolist()], n_results=5)\n",
    "        t5 = time.time()\n",
    "        print(f'Chroma add+query approx time: {(t5-t4):.3f}s (query part)')\n",
    "    except Exception as e:\n",
    "        print('Chroma perf test skipped ‚Äî chromadb not available or error:', e)\n",
    "except Exception as e:\n",
    "    print('Performance test skipped ‚Äî', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad92cd",
   "metadata": {},
   "source": [
    "## Best practices\n",
    "\n",
    "- **Choose chunk size** to preserve semantic units ‚Äî for contracts, larger chunks (500-1000 chars) often work better.\n",
    "- **Overlap** helps keep context between chunks: 50-200 characters or token-equivalent.\n",
    "- **Normalize embeddings** if you use cosine similarity with L2 indexes.\n",
    "- **Persist** FAISS indexes to disk and store metadata in a separate table (e.g., SQLite) if using FAISS in production.\n",
    "- **Test** retrieval quality with real queries before bulk ingestion.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
