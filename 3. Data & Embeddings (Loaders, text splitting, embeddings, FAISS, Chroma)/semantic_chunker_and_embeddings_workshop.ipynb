{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e4c3f0",
   "metadata": {},
   "source": [
    "# LangChain Text Splitters & Embeddings â€” Practical Workshop\n",
    "\n",
    "**Phase:** Semantic Chunking + Embeddings + Gemini demo ðŸ’¡ðŸ“ŒðŸ§ ðŸ”\n",
    "\n",
    "This notebook teaches practical splitters (including a semantic chunker you can use without `langchain_experimental`), shows how to generate embeddings, and demonstrates calling Gemini for contract-aware QA. It's Colab-ready and beginner friendly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce631ff",
   "metadata": {},
   "source": [
    "## Learning Guide\n",
    "\n",
    "**What you'll learn**\n",
    "\n",
    "- Why text splitting matters for legal/contract docs\n",
    "- Three practical splitters and a semantic chunker implementation (no `langchain_experimental` required)\n",
    "- How to get embeddings and compare similarity\n",
    "- How to call Gemini (Google Generative) with chunked context\n",
    "\n",
    "**Hands-on steps**\n",
    "\n",
    "1. Install dependencies if needed\n",
    "2. Load sample contract text\n",
    "3. Run character/recursive/token splitters\n",
    "4. Run the semantic chunker implemented here (embedding + greedy grouping)\n",
    "5. Generate embeddings with Gemini or SentenceTransformers\n",
    "6. Send best chunks to Gemini for an answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc857c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded (hidden)\n"
     ]
    }
   ],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n",
    "print('API key loaded (hidden)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e558e0",
   "metadata": {},
   "source": [
    "## Install / prerequisites\n",
    "\n",
    "Run the next cell if you need to install packages in Colab / your environment. It installs (optionally) langchain, sentence-transformers, sklearn, and the Google LangChain integration.\n",
    "\n",
    "It also avoids relying on `langchain_experimental` by providing a pure-Python semantic chunker implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "255f871c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded sample contract (1018 chars)\n"
     ]
    }
   ],
   "source": [
    "# Sample contract-like text for demonstration\n",
    "sample_contract = \"\"\"SERVICE AGREEMENT\n",
    "\n",
    "This Service Agreement (\"Agreement\") is made as of January 1, 2024, by and between Alpha Corp (\"Provider\") and Beta LLC (\"Client\").\n",
    "1. Services. Provider shall provide software development services (the \"Services\") described in Schedule A.\n",
    "2. Term. The term of this Agreement begins on the Effective Date and continues for twelve (12) months, unless earlier terminated.\n",
    "3. Payment. Client will pay Provider the fees set forth in Schedule B within thirty (30) days of invoice receipt.\n",
    "4. Confidentiality. Each party shall maintain confidential information in strict confidence and not disclose it to third parties.\n",
    "5. Liability. Neither party shall be liable for indirect or consequential damages.\n",
    "6. Termination. Either party may terminate upon thirty (30) days notice for material breach.\n",
    "7. Governing Law. This Agreement shall be governed by the laws of the State of Delaware.\n",
    "8. Miscellaneous. This Agreement constitutes the entire agreement between the parties and supersedes prior discussions.\n",
    "\"\"\"\n",
    "print('Loaded sample contract ({} chars)'.format(len(sample_contract)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e901ea00",
   "metadata": {},
   "source": [
    "## Classic LangChain Splitters\n",
    "\n",
    "The notebook will attempt to import LangChain splitters. If you don't have LangChain installed, either install it or skip to the semantic chunker (which does not require langchain_experimental)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d0eae1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --quiet langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fee06e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import LangChain splitters â€” No module named 'langchain.text_splitter'\n",
      "If you want them, run: pip install --quiet langchain\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, MarkdownHeaderTextSplitter, TokenTextSplitter\n",
    "    print('LangChain splitters imported.')\n",
    "except Exception as e:\n",
    "    print('Could not import LangChain splitters â€”', e)\n",
    "    print('If you want them, run: pip install --quiet langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9342db",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50, separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
    "    rec_chunks = splitter.split_text(sample_contract)\n",
    "    print('RecursiveCharacterTextSplitter ->', len(rec_chunks), 'chunks')\n",
    "    for i,c in enumerate(rec_chunks):\n",
    "        print(f'--- chunk {i+1} (len={len(c)}) ---\\n{c[:200]}\\n')\n",
    "except Exception as e:\n",
    "    print('Skipping recursive splitter:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "    char_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=40)\n",
    "    char_chunks = char_splitter.split_text(sample_contract)\n",
    "    print('CharacterTextSplitter ->', len(char_chunks), 'chunks')\n",
    "except Exception as e:\n",
    "    print('Skipping character splitter:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain.text_splitter import TokenTextSplitter\n",
    "    token_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=30)\n",
    "    token_chunks = token_splitter.split_text(sample_contract)\n",
    "    print('TokenTextSplitter ->', len(token_chunks), 'chunks')\n",
    "except Exception as e:\n",
    "    print('Skipping token splitter:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11717200",
   "metadata": {},
   "source": [
    "## SemanticChunker â€” custom implementation (no langchain_experimental)\n",
    "\n",
    "This cell implements a semantic chunker using `sentence-transformers` embeddings + a greedy grouping algorithm. The idea:\n",
    "\n",
    "1. Split the document into sentences.\n",
    "2. Compute sentence embeddings.\n",
    "3. Greedily group consecutive sentences until the group's embedding similarity to the next sentence is low or group reaches max characters.\n",
    "\n",
    "This approach preserves clause boundaries and groups semantically coherent text without requiring a special LangChain experimental package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticChunker implementation (greedy grouping)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Simple sentence splitter\n",
    "def split_sentences(text):\n",
    "    # naive splitter that keeps clause structure; for production use a proper sentence tokenizer\n",
    "    sentences = re.split(r'(?<=[.!?\\n])\\s+', text.strip())\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "# cosine similarity\n",
    "def cosine_sim(a, b):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10))\n",
    "\n",
    "class SemanticChunker:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', max_chars=800, min_chars=200, sim_threshold=0.7):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.max_chars = max_chars\n",
    "        self.min_chars = min_chars\n",
    "        self.sim_threshold = sim_threshold\n",
    "    \n",
    "    def chunk(self, text):\n",
    "        sentences = split_sentences(text)\n",
    "        embeddings = self.model.encode(sentences, normalize_embeddings=True)\n",
    "        chunks = []\n",
    "        cur_sentences = []\n",
    "        cur_embs = []\n",
    "        for i, sent in enumerate(sentences):\n",
    "            cur_sentences.append(sent)\n",
    "            cur_embs.append(embeddings[i])\n",
    "            cur_text = ' '.join(cur_sentences)\n",
    "            # stop growing if too long\n",
    "            if len(cur_text) >= self.max_chars:\n",
    "                chunks.append(cur_text)\n",
    "                cur_sentences = []\n",
    "                cur_embs = []\n",
    "                continue\n",
    "            # lookahead similarity: if next sentence is dissimilar, close the chunk\n",
    "            if i+1 < len(sentences):\n",
    "                next_emb = embeddings[i+1]\n",
    "                # compute centroid of current group\n",
    "                centroid = np.mean(np.vstack(cur_embs), axis=0)\n",
    "                sim = cosine_sim(centroid, next_emb)\n",
    "                if sim < self.sim_threshold and len(cur_text) >= self.min_chars:\n",
    "                    chunks.append(cur_text)\n",
    "                    cur_sentences = []\n",
    "                    cur_embs = []\n",
    "        # flush remaining\n",
    "        if cur_sentences:\n",
    "            chunks.append(' '.join(cur_sentences))\n",
    "        return chunks\n",
    "\n",
    "# Demo\n",
    "print('Loading sentence-transformers model (may download if not cached)...')\n",
    "chunker = SemanticChunker(max_chars=600, min_chars=120, sim_threshold=0.65)\n",
    "sem_chunks = chunker.chunk(sample_contract)\n",
    "print('SemanticChunker produced', len(sem_chunks), 'chunks')\n",
    "for i,c in enumerate(sem_chunks):\n",
    "    print(f'--- chunk {i+1} (len={len(c)}) ---\\n{c}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf6cc4",
   "metadata": {},
   "source": [
    "## Sending semantic chunks to Gemini (example)\n",
    "\n",
    "The following cell demonstrates how you'd send the top chunk(s) to the Gemini model using the `langchain_google_genai.ChatGoogleGenerativeAI` wrapper (matching your earlier code). It includes graceful fallbacks if the package isn't installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a825117",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    model = ChatGoogleGenerativeAI(model='gemini-2.5-flash', google_api_key=API_KEY)\n",
    "    print('Model initialized!')\n",
    "    # choose top N chunks (e.g., 2) to provide as context\n",
    "    context = \"\\n\\n\".join(sem_chunks[:2])\n",
    "    prompt = f\"\"\"You are a legal assistant. Using the context below from a service agreement, answer succinctly.\\nContext:\\n{context}\\n\\nQuestion: Summarize the key obligations of the Provider and Client in 3 bullet points.\"\"\"\n",
    "    response = model.invoke(prompt)\n",
    "    print('\\n=== Model response ===\\n')\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print('Could not run Gemini invocation â€”', e)\n",
    "    print('Make sure langchain-google-genai is installed and that API_KEY is valid.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3c49c",
   "metadata": {},
   "source": [
    "## Part 3 â€” Embeddings (Gemini + local alternatives)\n",
    "\n",
    "This section shows how to get embeddings using either Gemini (via LangChain's GoogleGenerativeAIEmbeddings) or locally using `sentence-transformers`. Use Gemini embeddings in prod if you prefer provider-managed vectors; use sentence-transformers for offline demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b567470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini embeddings (requires langchain_google_genai)\n",
    "try:\n",
    "    from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "    gem_emb = GoogleGenerativeAIEmbeddings(model='models/embedding-001', google_api_key=API_KEY)\n",
    "    texts = ['Terminate the contract immediately.', 'End the agreement right away.', 'The weather is sunny today.']\n",
    "    vectors = gem_emb.embed_documents(texts)\n",
    "    print('Got', len(vectors), 'embeddings from Gemini (length of first):', len(vectors[0]))\n",
    "except Exception as e:\n",
    "    print('Gemini embeddings not available â€”', e)\n",
    "    print('Falling back to sentence-transformers for local embedding demo.')\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "    st = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    texts = ['Terminate the contract immediately.', 'End the agreement right away.', 'The weather is sunny today.']\n",
    "    vectors = st.encode(texts, normalize_embeddings=True)\n",
    "    def cosine_sim(a,b): \n",
    "        return float(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)+1e-10))\n",
    "    print('Local embeddings ready. Similarities:')\n",
    "    print('text1 vs text2:', cosine_sim(vectors[0], vectors[1]))\n",
    "    print('text1 vs text3:', cosine_sim(vectors[0], vectors[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab7140",
   "metadata": {},
   "source": [
    "## Best practices & troubleshooting\n",
    "\n",
    "- If you saw `ModuleNotFoundError: No module named 'langchain_experimental'`, this notebook's SemanticChunker bypasses that by using `sentence-transformers` directly.\n",
    "- For contracts, prefer RecursiveCharacterTextSplitter or the SemanticChunker above with larger `max_chars` and moderate `sim_threshold` (0.6-0.75) to avoid breaking clauses.\n",
    "- Validate chunk outputs manually for the first few documents before bulk ingestion.\n",
    "- Use TokenTextSplitter or a final token-count pass when you must guarantee token budget limits for your LLM.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
