{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a6a913",
   "metadata": {},
   "source": [
    "#  ChatPromptTemplate\n",
    "###  Phase 2: Understanding and Using LangChain ChatPromptTemplate\n",
    "Learn how structured prompting works, how templates are constructed, and how dynamic content is inserted.\n",
    "Mastering prompt templates is essential for building reliable and scalable LLM workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d0a04",
   "metadata": {},
   "source": [
    "##  Learning Guide\n",
    "In this notebook, you will:\n",
    "- Understand what `ChatPromptTemplate` is and why it matters.\n",
    "- Explore different types of prompt templates.\n",
    "- Learn how dynamic variables like `{context}` and `{question}` make prompts flexible.\n",
    "- Practice hands-on examples.\n",
    "\n",
    "This lesson is part of the LangChain Foundations course and prepares you for building full LLM pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecffdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61d76a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-google-genai google-generativeai langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4fca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Selvam Sabarish\\AppData\\Local\\Python\\pythoncore-3.12-64\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000028130CC19D0>, default_metadata=())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash',\n",
    "    google_api_key=API_KEY\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015cdc1",
   "metadata": {},
   "source": [
    "## 1 Definition & Purpose\n",
    "`ChatPromptTemplate` is a LangChain component that allows you to create **structured prompts** with placeholders.\n",
    "\n",
    "### Why use it?\n",
    "- Ensures consistency\n",
    "- Reduces errors\n",
    "- Makes prompts dynamic and reusable\n",
    "- Standardizes LLM interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff17f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the concept of Neural networks in simple terms.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful AI assistant.'),\n",
    "    ('human', 'Explain the concept of {topic} in simple terms.')\n",
    "])\n",
    "\n",
    "filled = template.format_messages(topic='Neural networks')\n",
    "print(filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872476d8",
   "metadata": {},
   "source": [
    "## 2 Types of Prompt Templates\n",
    "- **System message**  Defines behavior or rules.\n",
    "- **Human message**  The users query.\n",
    "- **AI message**  Model-generated message used in multi-turn setups.\n",
    "- **Mixed multi-turn templates**  Combine several message roles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8956ae6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a reasoning engine.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Context: The Eiffel Tower is in Paris.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Question: Where is the Eiffel Tower located?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a reasoning engine.'),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}')\n",
    "])\n",
    "\n",
    "msg = multi_template.format_messages(\n",
    "    context='The Eiffel Tower is in Paris.',\n",
    "    question='Where is the Eiffel Tower located?'\n",
    ")\n",
    "msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58309354",
   "metadata": {},
   "source": [
    "## 3 Dynamic Prompting\n",
    "Dynamic variables make prompts powerful.\n",
    "\n",
    "Examples of placeholders:\n",
    "- `{context}`\n",
    "- `{question}`\n",
    "\n",
    "You can also set defaults or handle missing variables gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcda4240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Summarize this: LangChain provides modular abstractions for LLM applications.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyn_template = ChatPromptTemplate.from_messages([\n",
    "    ('human', 'Summarize this: {text}')\n",
    "])\n",
    "\n",
    "text = 'LangChain provides modular abstractions for LLM applications.'\n",
    "\n",
    "dyn_template.format_messages(text=text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d29cd",
   "metadata": {},
   "source": [
    "## 4 Best Practices\n",
    "- Provide **clear instructions**.\n",
    "- Add **constraints and guardrails**.\n",
    "- Format prompts with lists, bullets, or steps.\n",
    "- Keep templates clean and maintainable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcbb90",
   "metadata": {},
   "source": [
    "##  Summary\n",
    "In this notebook, you learned how to:\n",
    "- Build prompt templates\n",
    "- Use message types\n",
    "- Inject dynamic variables\n",
    "- Apply structured prompting principles\n",
    "\n",
    "You're now ready to build advanced LLM workflows!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
