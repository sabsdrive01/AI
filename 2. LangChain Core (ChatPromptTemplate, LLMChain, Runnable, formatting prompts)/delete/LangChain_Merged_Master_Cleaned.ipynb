{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1573378b",
   "metadata": {},
   "source": [
    "#  Learning Guide\n",
    "Welcome to **Introduction to LangChain Core**!\n",
    "\n",
    " **What this notebook teaches:**\n",
    "- The fundamentals of LangChain Core\n",
    "- Why LangChain is powerful for modern AI development\n",
    "- Core concepts: prompts, models, chains, runnables, parsers\n",
    "\n",
    " **Hands-on skills you'll gain:**\n",
    "- Initializing Gemini models via LangChain\n",
    "- Creating simple prompt-based interactions\n",
    "- Understanding how LangChain structures LLM applications\n",
    "\n",
    " **How it fits into the larger course:**\n",
    "This notebook sets the foundation for everything ahead. Before building agents, tools, RAG systems, or workflows, you must understand the LangChain Core building blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17083495",
   "metadata": {},
   "source": [
    "# 1 What is LangChain Core?\n",
    "LangChain Core is the essential framework layer used to build applications powered by Large Language Models (LLMs). It gives developers a clean and structured way to:\n",
    "\n",
    "- Build **prompts**\n",
    "- Connect to **models**\n",
    "- Build **chains** that combine multiple steps\n",
    "- Use **runnables** to assemble pipelines\n",
    "\n",
    " In short, LangChain Core is the *backbone* for organizing LLM workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ac7c3",
   "metadata": {},
   "source": [
    "## 2 Why LangChain is Useful for AI Applications\n",
    "LangChain brings powerful features that make development faster and more reliable:\n",
    "\n",
    "###  Standardization\n",
    "Consistent interfaces for models, prompts, and chains.\n",
    "\n",
    "###  Reusability\n",
    "Re-use components across many projects.\n",
    "\n",
    "###  Scalability\n",
    "Easily expand from simple prompts to full pipelines.\n",
    "\n",
    "###  Modularity\n",
    "Build complex systems by combining small logical blocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ea59d",
   "metadata": {},
   "source": [
    "## 3 Key Concepts Overview\n",
    "Lets break down the essential pieces of LangChain Core:\n",
    "\n",
    "###  Prompts\n",
    "Templates that define how information is sent to the model.\n",
    "\n",
    "###  Chains\n",
    "Sequences of steps where the output of one component becomes the input to another.\n",
    "\n",
    "###  Tools & Runnables\n",
    "Tools allow LLMs to interact with external systems.\n",
    "Runnables let you build reusable pipelines.\n",
    "\n",
    "###  Outputs & Parsers\n",
    "Parsers transform raw LLM output into structured formats like JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3132844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-google-genai google-generativeai langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43bddd",
   "metadata": {},
   "source": [
    "##  Hands-On Demo: Simple LangChain Prompt\n",
    "Let's try a basic interaction with Gemini through LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69614f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Selvam Sabarish\\AppData\\Local\\Python\\pythoncore-3.12-64\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized! üöÄ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangChain Core provides the foundational, standardized interface and primitives for interacting with and composing large language models into applications.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run--dcb8c5f4-d2c1-4e02-8ea5-74e248134ee1-0', usage_metadata={'input_tokens': 9, 'output_tokens': 22, 'total_tokens': 1146, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=API_KEY\n",
    ")\n",
    "print(\"Model initialized! \")\n",
    "\n",
    "response = model.invoke(\"Explain LangChain Core in one sentence.\")\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66ccec",
   "metadata": {},
   "source": [
    "#  Summary\n",
    "Great work! You explored:\n",
    "- What LangChain Core is\n",
    "- Why it matters for AI development\n",
    "- The key components like prompts, chains, and runnables\n",
    "\n",
    "You're now ready to explore more advanced LangChain capabilities!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a6a913",
   "metadata": {},
   "source": [
    "#  ChatPromptTemplate\n",
    "###  Phase 2: Understanding and Using LangChain ChatPromptTemplate\n",
    "Learn how structured prompting works, how templates are constructed, and how dynamic content is inserted.\n",
    "Mastering prompt templates is essential for building reliable and scalable LLM workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d0a04",
   "metadata": {},
   "source": [
    "##  Learning Guide\n",
    "In this notebook, you will:\n",
    "- Understand what `ChatPromptTemplate` is and why it matters.\n",
    "- Explore different types of prompt templates.\n",
    "- Learn how dynamic variables like `{context}` and `{question}` make prompts flexible.\n",
    "- Practice hands-on examples.\n",
    "\n",
    "This lesson is part of the LangChain Foundations course and prepares you for building full LLM pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecffdfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61d76a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-google-genai google-generativeai langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4fca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Selvam Sabarish\\AppData\\Local\\Python\\pythoncore-3.12-64\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x0000028130CC19D0>, default_metadata=())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash',\n",
    "    google_api_key=API_KEY\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015cdc1",
   "metadata": {},
   "source": [
    "## 1 Definition & Purpose\n",
    "`ChatPromptTemplate` is a LangChain component that allows you to create **structured prompts** with placeholders.\n",
    "\n",
    "### Why use it?\n",
    "- Ensures consistency\n",
    "- Reduces errors\n",
    "- Makes prompts dynamic and reusable\n",
    "- Standardizes LLM interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff17f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful AI assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the concept of Neural networks in simple terms.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful AI assistant.'),\n",
    "    ('human', 'Explain the concept of {topic} in simple terms.')\n",
    "])\n",
    "\n",
    "filled = template.format_messages(topic='Neural networks')\n",
    "print(filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872476d8",
   "metadata": {},
   "source": [
    "## 2 Types of Prompt Templates\n",
    "- **System message**  Defines behavior or rules.\n",
    "- **Human message**  The users query.\n",
    "- **AI message**  Model-generated message used in multi-turn setups.\n",
    "- **Mixed multi-turn templates**  Combine several message roles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8956ae6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a reasoning engine.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Context: The Eiffel Tower is in Paris.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Question: Where is the Eiffel Tower located?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a reasoning engine.'),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}')\n",
    "])\n",
    "\n",
    "msg = multi_template.format_messages(\n",
    "    context='The Eiffel Tower is in Paris.',\n",
    "    question='Where is the Eiffel Tower located?'\n",
    ")\n",
    "msg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58309354",
   "metadata": {},
   "source": [
    "## 3 Dynamic Prompting\n",
    "Dynamic variables make prompts powerful.\n",
    "\n",
    "Examples of placeholders:\n",
    "- `{context}`\n",
    "- `{question}`\n",
    "\n",
    "You can also set defaults or handle missing variables gracefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcda4240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Summarize this: LangChain provides modular abstractions for LLM applications.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyn_template = ChatPromptTemplate.from_messages([\n",
    "    ('human', 'Summarize this: {text}')\n",
    "])\n",
    "\n",
    "text = 'LangChain provides modular abstractions for LLM applications.'\n",
    "\n",
    "dyn_template.format_messages(text=text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d29cd",
   "metadata": {},
   "source": [
    "## 4 Best Practices\n",
    "- Provide **clear instructions**.\n",
    "- Add **constraints and guardrails**.\n",
    "- Format prompts with lists, bullets, or steps.\n",
    "- Keep templates clean and maintainable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dcbb90",
   "metadata": {},
   "source": [
    "##  Summary\n",
    "In this notebook, you learned how to:\n",
    "- Build prompt templates\n",
    "- Use message types\n",
    "- Inject dynamic variables\n",
    "- Apply structured prompting principles\n",
    "\n",
    "You're now ready to build advanced LLM workflows!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0c9337",
   "metadata": {},
   "source": [
    "# LLMChain  Structured Workflows with LLMs\n",
    "\n",
    "## Understanding How Prompts + Models Form Powerful AI Pipelines\n",
    "This notebook introduces **LLMChain**, a core abstraction that connects prompts and language models to create clean, repeatable workflows.\n",
    "You'll explore how prompts, models, and output parsers work together and build hands-on examples to master the concept!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec544a6",
   "metadata": {},
   "source": [
    "##  Learning Guide\n",
    "\n",
    "In this notebook, you will learn:\n",
    "\n",
    "- What **LLMChain** is and why it's essential in any LLM-powered application.\n",
    "- How prompts, models, and output parsers connect together in a pipeline.\n",
    "- How LLMChain fits into the broader system of prompt engineering and chain-based architectures.\n",
    "- How to build simple single-step workflows that can later scale into multi-step chains.\n",
    "\n",
    "Youll perform hands-on tasks such as:\n",
    "\n",
    "- Creating prompt templates\n",
    "- Passing data dynamically\n",
    "- Initializing LLMChain-like logic\n",
    "- Running test generations\n",
    "\n",
    "This is a foundational skill that strengthens your entire AI engineering journey.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bc3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c2805",
   "metadata": {},
   "source": [
    "# **3. LLMChain**\n",
    "\n",
    "LLMChain is one of the most important building blocks in LLM applications.\n",
    "It structures the flow of information so developers can repeatedly execute:\n",
    "\n",
    "### **PROMPT  MODEL  OUTPUT**\n",
    "\n",
    "---\n",
    "\n",
    "##  1. What Is LLMChain?\n",
    "\n",
    "LLMChain is a simple pipeline where:\n",
    "\n",
    "- You define a **prompt**\n",
    "- You pass it to an **LLM** (GPT, Gemini, Claude, etc.)\n",
    "- You receive an **output**\n",
    "\n",
    "It provides a clean interface to consistently run queries with predictable formatting.\n",
    "\n",
    "---\n",
    "\n",
    "##  2. Components of LLMChain\n",
    "\n",
    "### **1. PromptTemplate**\n",
    "A reusable text template with placeholders like:\n",
    "`\"Explain {topic} in simple terms.\"`\n",
    "\n",
    "### **2. LLM**\n",
    "The model that generates the completion.\n",
    "\n",
    "### **3. Output Parser**\n",
    "Processes the raw model output into structured data, such as:\n",
    "- JSON\n",
    "- extracted fields\n",
    "- cleaned text\n",
    "\n",
    "Together, these components form a powerful abstraction.\n",
    "\n",
    "---\n",
    "\n",
    "##  3. Use Cases of LLMChain\n",
    "\n",
    "- **Question answering**\n",
    "- **Text generation**\n",
    "- **Data extraction**\n",
    "- **Document summarization**\n",
    "- **Knowledge retrieval**\n",
    "\n",
    "Any task that follows:\n",
    "**Input  Model Reasoning  Output**\n",
    "can be wrapped inside an LLMChain.\n",
    "\n",
    "---\n",
    "\n",
    "##  4. Advantages of LLMChain\n",
    "\n",
    "- **Reusable:** The same chain can run thousands of executions.\n",
    "- **Configurable:** Swap prompts, models, parameters effortlessly.\n",
    "- **Composable:** Can be part of larger multi-step workflows.\n",
    "- **Maintainable:** Centralizes logic into a clean, declarative structure.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0f693",
   "metadata": {},
   "source": [
    "#  Runnable Interface in LangChain\n",
    "###  Understanding LangChains Modern Execution Engine\n",
    "\n",
    "This notebook provides a structured, beginnerfriendly walkthrough of LangChains `Runnable` interface  a powerful and flexible way to build LLM-powered pipelines. Youll explore concepts, examples, and handson demos!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55123788",
   "metadata": {},
   "source": [
    "##  Learning Guide\n",
    "In this notebook, you will learn:\n",
    "- What the `Runnable` interface is and why it powers LangChains new architecture\n",
    "- How different runnable types (Map, Sequence, Lambda, Branch) work\n",
    "- How to build composable and scalable workflows\n",
    "- How sync & async execution models improve performance\n",
    "\n",
    "###  Why This Matters\n",
    "`Runnable` is the *core execution engine* in LangChain, making chains more flexible, powerful, and modular. Mastering it is essential for building production-grade LLM applications.\n",
    "\n",
    "###  Hands-on Steps You Will Perform\n",
    "- Initialize Gemini via LangChain\n",
    "- Build runnable sequences\n",
    "- Test different runnable types\n",
    "- Compose pipelines end-to-end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c8fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a08d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain langchain-google-genai google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b556b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Selvam Sabarish\\AppData\\Local\\Python\\pythoncore-3.12-64\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatGoogleGenerativeAI(model='models/gemini-2.0-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001EB547C5940>, default_metadata=())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash',\n",
    "    google_api_key=API_KEY\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ea215",
   "metadata": {},
   "source": [
    "#  1. What Is a Runnable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02720e04",
   "metadata": {},
   "source": [
    "`Runnable` is LangChains universal execution abstraction. **Everything becomes a runnable**:\n",
    "- Prompts\n",
    "- Models\n",
    "- Chains\n",
    "- Lambdas (functions)\n",
    "- Maps (parallel execution)\n",
    "- Sequences (pipeline execution)\n",
    "\n",
    "This makes it incredibly easy to compose complex LLM workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4619f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, let's break down quantum computing in a way that's understandable. It's a complex topic, but we can cover the key concepts.\\n\\n**What is Quantum Computing? (In a Nutshell)**\\n\\nQuantum computing is a new type of computation that leverages the principles of quantum mechanics to solve problems that are too complex for classical computers. Instead of using bits that are either 0 or 1, quantum computers use **qubits** that can be 0, 1, or *both* simultaneously. This allows them to explore many possibilities at once, potentially leading to exponential speedups for certain types of calculations.\\n\\n**Key Concepts Explained:**\\n\\n1. **Classical Bits vs. Quantum Bits (Qubits):**\\n\\n   *   **Classical Bits:**  The fundamental unit of information in a classical computer.  A bit can be either a 0 or a 1.  Think of it like a light switch that's either on or off.\\n\\n   *   **Qubits:** The fundamental unit of information in a quantum computer.  Unlike bits, qubits can exist in a superposition of states.  Imagine a light switch that can be both on and off *at the same time*.  This is achieved through the weirdness of quantum mechanics.\\n\\n2. **Superposition:**\\n\\n   *   This is the ability of a qubit to be in a combination of 0 and 1 simultaneously.  It's not just a 50/50 probability; it's a true, blended state.  Think of it like a coin spinning in the air. It's neither heads nor tails until it lands.  Mathematically, a qubit's state is represented as a linear combination of 0 and 1:\\n\\n       `|œà‚ü© = Œ±|0‚ü© + Œ≤|1‚ü©`\\n\\n       Where:\\n       *   `|œà‚ü©` is the qubit's state.\\n       *   `|0‚ü©` represents the state 0.\\n       *   `|1‚ü©` represents the state 1.\\n       *   `Œ±` and `Œ≤` are complex numbers (amplitudes) that determine the probability of measuring the qubit as 0 or 1.  The squares of their magnitudes (`|Œ±|^2` and `|Œ≤|^2`) give the probabilities.\\n\\n3. **Entanglement:**\\n\\n   *   This is a spooky quantum phenomenon where two or more qubits become linked together in such a way that the state of one instantly influences the state of the others, no matter how far apart they are.  If you measure one entangled qubit and find it's 0, you instantly know the state of its entangled partner (it might be 1, or correlated in some other way, depending on the entanglement).\\n\\n   *   Entanglement is crucial for quantum algorithms because it allows qubits to work together to solve problems in a coordinated way.  Think of it as two spinning coins that are linked; when one lands on heads, the other instantly lands on tails (or vice-versa).\\n\\n4. **Quantum Gates:**\\n\\n   *   Just like classical computers use logic gates (AND, OR, NOT) to manipulate bits, quantum computers use quantum gates to manipulate qubits.  Quantum gates are mathematical operations (specifically, unitary transformations) that change the superposition and entanglement of qubits.\\n\\n   *   Examples of quantum gates include:\\n        *   **Hadamard gate (H):**  Puts a qubit into an equal superposition of 0 and 1.\\n        *   **CNOT gate (Controlled-NOT):**  Performs a NOT operation on one qubit (the target) based on the state of another qubit (the control). This is fundamental for creating entanglement.\\n        *   **Pauli Gates (X, Y, Z):**  Perform rotations around the X, Y, and Z axes on the Bloch sphere (a visual representation of a qubit's state).\\n\\n5. **Quantum Algorithms:**\\n\\n   *   These are specific sequences of quantum gates designed to solve particular problems.  Some famous quantum algorithms include:\\n\\n       *   **Shor's Algorithm:**  Can factor large numbers exponentially faster than the best-known classical algorithms.  This has implications for cryptography, as many encryption schemes rely on the difficulty of factoring.\\n\\n       *   **Grover's Algorithm:**  Provides a quadratic speedup for searching unsorted databases.\\n\\n       *   **Quantum Simulation:**  Allows simulating the behavior of quantum systems (like molecules or materials) with much greater accuracy than classical computers.  This has huge potential for drug discovery, materials science, and fundamental physics research.\\n\\n6. **Decoherence:**\\n\\n   *   This is a major challenge in quantum computing.  Qubits are very sensitive to their environment.  Any interaction with the outside world (noise, vibrations, temperature fluctuations) can cause them to lose their superposition and entanglement, leading to errors in the computation.  Maintaining the delicate quantum state of qubits is a significant engineering hurdle.\\n\\n**Why is Quantum Computing Important?**\\n\\nClassical computers are reaching their limits in solving certain types of problems. Quantum computers offer the potential to:\\n\\n*   **Break modern encryption:** Shor's algorithm poses a threat to widely used encryption algorithms.\\n*   **Discover new drugs and materials:** Quantum simulation can revolutionize drug discovery and materials science by accurately modeling molecular interactions.\\n*   **Optimize complex systems:** Quantum algorithms can be used to optimize things like traffic flow, financial models, and supply chains.\\n*   **Accelerate machine learning:** Quantum machine learning algorithms could lead to breakthroughs in areas like image recognition and natural language processing.\\n*   **Solve previously intractable scientific problems:** Quantum computing can tackle problems in fields like fundamental physics, cosmology, and climate modeling.\\n\\n**Challenges in Quantum Computing:**\\n\\n*   **Decoherence:** Maintaining the delicate quantum state of qubits is extremely difficult.\\n*   **Scalability:** Building quantum computers with a large number of qubits is a major engineering challenge.\\n*   **Error Correction:** Quantum computations are prone to errors, and developing robust error correction techniques is crucial.\\n*   **Algorithm Development:**  We need to develop more quantum algorithms to take advantage of the power of quantum computers.\\n*   **Hardware Development:**  Different types of qubits (superconducting, trapped ions, photons, etc.) each have their own advantages and disadvantages, and developing reliable and scalable quantum hardware is an ongoing effort.\\n\\n**Different Types of Quantum Computers (Hardware):**\\n\\n*   **Superconducting Qubits:**  Use superconducting circuits to create qubits.  Companies like Google, IBM, and Rigetti are developing superconducting quantum computers.\\n*   **Trapped Ions:**  Use individual ions (charged atoms) trapped by electromagnetic fields as qubits.  Companies like IonQ and Quantinuum are pursuing trapped ion technology.\\n*   **Photonic Qubits:**  Use photons (particles of light) as qubits.  Companies like Xanadu are working on photonic quantum computers.\\n*   **Neutral Atoms:**  Use neutral atoms trapped in optical lattices as qubits.\\n*   **Silicon Qubits:** Use electron spin in silicon-based devices as qubits.\\n\\n**In Summary:**\\n\\nQuantum computing is a revolutionary approach to computation that harnesses the principles of quantum mechanics to solve problems beyond the reach of classical computers. While still in its early stages of development, it holds immense potential to transform fields ranging from medicine and materials science to cryptography and artificial intelligence. The key concepts to understand are qubits, superposition, entanglement, and decoherence.  It's a challenging field, but the potential rewards are enormous.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableSequence\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Explain: {topic}\")\n",
    "\n",
    "chain = RunnableSequence(prompt,model)\n",
    "\n",
    "chain\n",
    "\n",
    "response = chain.invoke({\"topic\": \"quantum computing\"})\n",
    "\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a602036",
   "metadata": {},
   "source": [
    "#  2. Types of Runnables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbfc19",
   "metadata": {},
   "source": [
    "## 2.1 `RunnableLambda`\n",
    "A lightweight wrapper around a Python function.\n",
    "Perfect for preprocessing, validation, or custom logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d66b81fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO RUNNABLE!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper = RunnableLambda(lambda x: x.upper())\n",
    "upper.invoke(\"hello runnable!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484ce97",
   "metadata": {},
   "source": [
    "## 2.2 `RunnableMap`\n",
    "Run multiple tasks in **parallel**  powerful for multi-extraction tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a1fade9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'LangChain makes LLM workflows modular and composab', 'length': 53}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "process = RunnableMap({\n",
    "    \"summary\": RunnableLambda(lambda x: x[:50]),\n",
    "    \"length\": RunnableLambda(lambda x: len(x)),\n",
    "})\n",
    "\n",
    "process.invoke(\"LangChain makes LLM workflows modular and composable!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675cb72",
   "metadata": {},
   "source": [
    "## 2.3 `RunnableSequence`\n",
    "Create linear pipelines: **step  step  step**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e11d5a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Processed: Welcome to Bulerez! ‚úîÔ∏è'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = RunnableSequence(\n",
    "    RunnableLambda(lambda x: f\"Processed: {x}\"),\n",
    "    RunnableLambda(lambda x: x + \" \"),\n",
    ")\n",
    "\n",
    "sequence.invoke(\"Welcome to Bulerez!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74e31b",
   "metadata": {},
   "source": [
    "## 2.4 `RunnableBranch`\n",
    "Conditional branching logic  choose different pipelines based on input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edcc3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greeting detected!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"hello\" in x.lower(), RunnableLambda(lambda x: \"Greeting detected!\")),\n",
    "    RunnableLambda(lambda x: \"No greeting found.\"),\n",
    ")\n",
    "\n",
    "branch.invoke(\"Hello there!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb1671",
   "metadata": {},
   "source": [
    "#  3. Async vs Sync Execution\n",
    "\n",
    "Runnables support two powerful execution modes  **Sync** and **Async**  enabling flexibility for both simple and high-performance AI workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d815b85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  Sync vs Async in LangChain  Beginner-Friendly Explanation\n",
    "\n",
    "LangChain Runnables can be executed in **two different ways**, depending on your needs:\n",
    "\n",
    "* **Sync**  Traditional, simple, step-by-step execution\n",
    "* **Async**  Non-blocking, parallel, faster multi-call execution\n",
    "\n",
    "Lets break them down clearly.\n",
    "\n",
    "---\n",
    "\n",
    "### 1 Sync Mode  `.invoke()`\n",
    "\n",
    "Sync = **You call  You wait  You get the output**.\n",
    "\n",
    "```\n",
    "You  chain.invoke()  wait  result\n",
    "```\n",
    "\n",
    "### When to Use Sync Mode\n",
    "\n",
    "* Basic testing\n",
    "* Running one LLM call at a time\n",
    "* Simple scripts\n",
    "* Learning or debugging\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "result = chain.invoke({\"topic\": \"quantum computing\"})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2 Async Mode  `.ainvoke()`\n",
    "\n",
    "Async = **Non-blocking execution**.\n",
    "Your program doesn't pause while waiting for the model response.\n",
    "\n",
    "This means you can:\n",
    "\n",
    "* Run **multiple LLM calls at the same time**\n",
    "* Speed up heavy pipelines\n",
    "* Build fast backend services\n",
    "\n",
    "### Real-Life Analogy\n",
    "\n",
    "Async is like ordering food at multiple counters simultaneously:\n",
    "\n",
    "```\n",
    "You  send many orders  each gets prepared in parallel  results return faster\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  Example: Async `.ainvoke()` (Beginner-Friendly)\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "# Define an async function\n",
    "async def demo():\n",
    "    # 'ainvoke' runs the chain without blocking the program\n",
    "    result = await chain.ainvoke({\"topic\": \"async execution in LangChain\"})\n",
    "    print(result)\n",
    "\n",
    "# Run the async function\n",
    "await demo()\n",
    "```\n",
    "\n",
    "---\n",
    "###  Whats Happening Here?\n",
    "\n",
    "* **`async def demo():`**\n",
    "  Creates an async function that can run asynchronous tasks.\n",
    "\n",
    "* **`await chain.ainvoke({...})`**\n",
    "  Calls the LLM *without blocking* your program.\n",
    "\n",
    "* **`await demo()`**\n",
    "  Executes the async function.\n",
    "\n",
    "This allows your application to remain fast and responsive  even during multiple model calls.\n",
    "\n",
    "---\n",
    "\n",
    "###  Summary (Simple & Clear)\n",
    "\n",
    "| Mode      | Method       | When to Use                    | Behavior                    |\n",
    "| --------- | ------------ | ------------------------------ | --------------------------- |\n",
    "| **Sync**  | `.invoke()`  | Simple or single tasks         | Blocking (waits for result) |\n",
    "| **Async** | `.ainvoke()` | Many calls, speed, parallelism | Non-blocking (much faster)  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197fae1",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02f68f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Asynchronous execution in LangChain allows you to run tasks concurrently, improving the overall performance and efficiency of your applications.  It\\'s particularly beneficial when dealing with operations that involve waiting, such as:\\n\\n*   **API calls:** Querying external APIs (like LLMs) often involves network latency.\\n*   **File I/O:** Reading from or writing to files can take time.\\n*   **Other I/O-bound operations:** Any task that spends a significant amount of time waiting for external resources.\\n\\nBy using asynchronous execution, your LangChain application can perform other tasks while waiting for these operations to complete, rather than blocking and idling.\\n\\nHere\\'s a breakdown of how async execution works in LangChain and why it\\'s useful:\\n\\n**Key Concepts**\\n\\n*   **`async` and `await`:** These are keywords in Python that enable asynchronous programming.  `async` declares a function as a coroutine, which can be paused and resumed.  `await` is used to pause the execution of a coroutine until an awaitable object (like another coroutine or a `Future`) completes.\\n*   **Event Loop:** Python\\'s `asyncio` library uses an event loop to manage the execution of asynchronous tasks.  The event loop monitors the status of tasks and switches between them when they are ready to proceed.\\n*   **Concurrency vs. Parallelism:** It\\'s important to understand the difference.  Concurrency means that multiple tasks can make progress seemingly at the same time, by interleaving their execution.  Parallelism means that multiple tasks are actually running simultaneously on different CPU cores.  `asyncio` in Python provides concurrency, but it doesn\\'t necessarily provide parallelism (unless you use other libraries like `multiprocessing` in conjunction with `asyncio`).\\n\\n**How LangChain Implements Async**\\n\\nLangChain leverages Python\\'s `asyncio` library to provide asynchronous versions of many of its core components.  This includes:\\n\\n*   **LLMs (Language Models):**  You can make asynchronous calls to language models using the `acomplete` method (or similar async counterparts) instead of the synchronous `complete` (or similar) method.  This allows you to send multiple prompts to the LLM concurrently.\\n*   **Chains:** You can create asynchronous chains where the steps within the chain are executed concurrently.  This is particularly useful if your chain involves multiple API calls or other I/O-bound operations.\\n*   **Retrieval:** Async versions of document loaders and vectorstores allow for faster document processing and retrieval.\\n*   **Agents:** Asynchronous execution in agents enables faster decision-making and tool execution.\\n*   **Callbacks:**  Callbacks can also be asynchronous, allowing you to perform operations like logging or streaming results without blocking the main execution flow.\\n\\n**Example (Illustrative)**\\n\\n```python\\nimport asyncio\\nfrom langchain.llms import OpenAI\\n\\nasync def main():\\n    llm = OpenAI(temperature=0)\\n\\n    # Asynchronous calls\\n    tasks = [\\n        llm.agenerate([\"Tell me a joke.\"]),\\n        llm.agenerate([\"What is the capital of France?\"])\\n    ]\\n\\n    results = await asyncio.gather(*tasks)  # Execute tasks concurrently\\n\\n    print(results[0].generations[0].text)\\n    print(results[1].generations[0].text)\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\\n```\\n\\n**Explanation of the Example:**\\n\\n1.  **`async def main():`**:  Defines an asynchronous function called `main`.  This is the entry point for our asynchronous code.\\n2.  **`llm = OpenAI(temperature=0)`**: Creates an instance of the OpenAI language model.\\n3.  **`tasks = [...]`**:  Creates a list of asynchronous tasks.  `llm.agenerate()` is the asynchronous version of the `generate()` method.  Each task represents a call to the language model.\\n4.  **`results = await asyncio.gather(*tasks)`**: This is the key part. `asyncio.gather()` takes a list of awaitable objects (in this case, the tasks) and runs them concurrently.  The `await` keyword pauses the execution of `main` until all the tasks in `gather` are complete.  The results of each task are returned in a list.\\n5.  **`print(...)`**: Prints the results from each task.\\n6.  **`asyncio.run(main())`**:  This line runs the `main` coroutine.  It creates an event loop, runs the coroutine until it completes, and then closes the event loop.\\n\\n**Benefits of Asynchronous Execution**\\n\\n*   **Improved Performance:** By running tasks concurrently, you can reduce the overall execution time of your LangChain applications, especially when dealing with I/O-bound operations.\\n*   **Increased Responsiveness:** Asynchronous execution prevents your application from blocking while waiting for tasks to complete. This makes your application more responsive to user input and other events.\\n*   **Better Resource Utilization:** Asynchronous execution allows you to make better use of your system\\'s resources, as the CPU can be used to perform other tasks while waiting for I/O operations to complete.\\n*   **Scalability:** Asynchronous execution can help you build more scalable LangChain applications, as you can handle more concurrent requests without blocking.\\n\\n**When to Use Asynchronous Execution**\\n\\n*   **I/O-Bound Operations:**  Use asynchronous execution whenever your LangChain application involves a significant amount of I/O, such as making API calls, reading from files, or accessing databases.\\n*   **Concurrency Requirements:** If you need to handle multiple requests concurrently, asynchronous execution is a good choice.\\n*   **Performance Optimization:** If you want to optimize the performance of your LangChain application, consider using asynchronous execution to parallelize tasks.\\n\\n**Considerations**\\n\\n*   **Complexity:** Asynchronous programming can be more complex than synchronous programming. You need to understand the concepts of coroutines, event loops, and `async` / `await`.\\n*   **Debugging:** Debugging asynchronous code can be more challenging than debugging synchronous code.\\n*   **Library Support:**  Ensure that the libraries you are using (including LangChain components) have asynchronous versions available.\\n\\n**In Summary**\\n\\nAsynchronous execution in LangChain is a powerful technique for improving the performance, responsiveness, and scalability of your applications. By leveraging Python\\'s `asyncio` library, LangChain allows you to run tasks concurrently, reducing the overall execution time and making better use of system resources.  When dealing with I/O-bound operations or concurrency requirements, asynchronous execution is a valuable tool to consider. Remember to carefully consider the complexity and debugging challenges involved.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--5a33036c-b74d-413d-8056-ca58e5a2cefc-0' usage_metadata={'input_tokens': 7, 'output_tokens': 1440, 'total_tokens': 1447, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "async def demo():\n",
    "    result = await chain.ainvoke({\"topic\": \"async execution in LangChain\"})\n",
    "    print(result)\n",
    "\n",
    "await demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc08c7",
   "metadata": {},
   "source": [
    "#  4. Composability in Runnables\n",
    "Build full pipelines:\n",
    "`preprocess  prompt  model  postprocess`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3262b385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain pipelines, or chains, are a fundamental concept in the LangChain framework. They represent a sequence of calls to different components, linking them together to achieve a more complex task than any single component could accomplish on its own. Think of it like an assembly line where each station performs a specific operation, and the output of one station becomes the input of the next.\\n\\nHere\\'s a breakdown of what they are and why they are important:\\n\\n**What are LangChain Chains?**\\n\\n* **Sequences of Calls:** At their core, a chain is a sequence of calls, typically to Large Language Models (LLMs), but also to other components like data connectors, document loaders, vector databases, and more.\\n\\n* **Connecting Components:**  Chains allow you to connect these individual components in a logical order, defining the flow of information and processing.\\n\\n* **Modular and Reusable:** Chains are designed to be modular, meaning you can combine different chains to create even more complex workflows. They are also designed to be reusable, allowing you to apply the same chain to different inputs.\\n\\n* **Abstraction:** They abstract away the complexity of managing the interactions between different components, making it easier to build sophisticated applications.\\n\\n**Why use LangChain Chains?**\\n\\n* **Complexity Management:**  Breaking down complex tasks into smaller, manageable steps.  Instead of writing a single, monolithic function, you create a chain of functions, each responsible for a specific part of the process.\\n\\n* **Automation:** Automating tasks that would otherwise require manual intervention.  For example, you can create a chain that loads a document, extracts information from it, and then uses that information to answer questions.\\n\\n* **Flexibility:** Adapting to different inputs and outputs.  You can create chains that can handle different types of data, or that can produce different types of outputs.\\n\\n* **Reusability:**  Creating reusable components that can be used in multiple applications.  This saves time and effort, and also helps to ensure consistency across applications.\\n\\n* **Maintainability:**  Making your code easier to understand and maintain.  By breaking down complex tasks into smaller, more manageable steps, you make it easier to identify and fix problems.\\n\\n**Types of Chains (Illustrative Examples):**\\n\\nLangChain provides various pre-built chain types and also allows you to create custom chains.  Here are some common examples:\\n\\n* **LLMChain:**  The most fundamental chain. It takes an LLM, a prompt template, and optionally an output parser.  It formats the prompt template with the input variables and passes it to the LLM. The LLM\\'s output is then parsed by the output parser (if present).\\n\\n   ```python\\n   from langchain.llms import OpenAI\\n   from langchain.prompts import PromptTemplate\\n   from langchain.chains import LLMChain\\n\\n   llm = OpenAI(temperature=0.9)\\n   prompt = PromptTemplate(\\n       input_variables=[\"product\"],\\n       template=\"What is a good name for a company that makes {product}?\",\\n   )\\n\\n   chain = LLMChain(llm=llm, prompt=prompt)\\n   print(chain.run(\"colorful socks\")) # Example Usage\\n   ```\\n\\n* **SimpleSequentialChain:**  The simplest form of sequential chain. It takes a list of chains as input and runs them in order, passing the output of one chain to the next.  Good for simple workflows where the output of one step directly feeds into the next.\\n\\n* **SequentialChain:** A more general sequential chain. It allows you to specify different input and output keys for each chain, giving you more control over how the data flows.\\n\\n* **RouterChain:** A dynamic chain that selects which sub-chain to execute based on the input.  This is useful for creating conditional workflows where the execution path depends on the user\\'s input or the data being processed.  It typically uses an LLM to determine the best chain to use.\\n\\n* **Combine Documents Chain:**  Designed to handle large documents.  It splits the document into chunks, processes each chunk with an LLM (often using a map-reduce approach), and then combines the results.  Essential for tasks like summarization or question answering over long texts.\\n\\n**Key Concepts related to Chains:**\\n\\n* **LLMs (Large Language Models):** The AI models that generate text.  Chains frequently use LLMs to perform tasks like translation, summarization, question answering, and more.\\n* **Prompt Templates:**  Define the structure of the input that is sent to the LLM.  They allow you to dynamically inject variables into the prompt, making it more flexible and adaptable.\\n* **Output Parsers:**  Extract structured information from the LLM\\'s output.  They can convert the raw text output into a more usable format, such as a JSON object or a list of entities.\\n* **Memory:**  Chains can be designed to maintain state across multiple calls.  This is useful for conversational applications where you need to remember the context of the conversation.  LangChain provides various memory implementations.\\n* **Agents:**  Agents are a more advanced concept that build on chains.  They use an LLM to decide which action to take next, choosing from a set of available tools (which can include chains).  This allows agents to perform more complex and autonomous tasks.\\n\\n**Example Scenario:  Question Answering over a Document**\\n\\nImagine you want to build a system that can answer questions about a specific document.  You could use a LangChain chain to accomplish this:\\n\\n1. **Document Loading:**  Use a `DocumentLoader` to load the document into memory.\\n2. **Text Splitting:**  Split the document into smaller chunks using a `TextSplitter`.\\n3. **Vector Embedding:**  Create vector embeddings of the text chunks using an `Embeddings` model.\\n4. **Vector Database:**  Store the embeddings in a `VectorStore` (e.g., Chroma, FAISS).\\n5. **Retrieval Chain:**\\n    * Take the user\\'s question as input.\\n    * Embed the question using the same `Embeddings` model.\\n    * Use the `VectorStore` to find the most relevant text chunks based on similarity.\\n    * Create a prompt that includes the question and the relevant text chunks.\\n    * Pass the prompt to an `LLM` to generate an answer.\\n\\n**Benefits of Using LangChain\\'s Chain Abstraction:**\\n\\n* **Simplified Development:** LangChain provides a high-level abstraction that simplifies the process of building complex applications.  You don\\'t need to worry about the low-level details of managing the interactions between different components.\\n* **Improved Code Readability:** Chains make your code more readable and easier to understand.  By breaking down complex tasks into smaller, more manageable steps, you make it easier to follow the logic of your application.\\n* **Increased Code Reusability:** Chains are designed to be reusable.  You can create chains that can be used in multiple applications, saving time and effort.\\n* **Enhanced Collaboration:** Chains make it easier to collaborate with other developers.  By providing a common framework for building applications, LangChain makes it easier for developers to share code and knowledge.\\n\\nIn summary, LangChain chains are a powerful and flexible way to build complex applications using LLMs and other components. They provide a high-level abstraction that simplifies development, improves code readability, and increases code reusability. Understanding and utilizing chains is crucial for effectively leveraging the capabilities of LangChain.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = (\n",
    "    RunnableLambda(lambda x: {\"topic\": x.strip()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | RunnableLambda(lambda x: x.content)\n",
    ")\n",
    "\n",
    "pipeline.invoke(\"   LangChain pipelines   \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a48262",
   "metadata": {},
   "source": [
    "#  Summary\n",
    "You now understand:\n",
    "- What the Runnable interface is\n",
    "- Runnable types: Map, Sequence, Lambda, Branch\n",
    "- Sync vs async execution\n",
    "- How to compose powerful pipelines\n",
    "\n",
    "Youre now ready to build advanced LangChain workflows!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35104a6",
   "metadata": {},
   "source": [
    "# Prompt Formatting & Best Practices\n",
    "## Ensuring High-Quality, Stable, and Reliable LLM Outputs\n",
    "\n",
    "This notebook provides a clear and structured guide on how to design effective prompts.\n",
    "You'll learn formatting techniques, hallucination reduction strategies, and how to\n",
    "evaluate prompts using industry best practices. This is essential for anyone building\n",
    "robust AI workflows and enterprise-grade LLM systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f209e",
   "metadata": {},
   "source": [
    "## Learning Guide\n",
    "In this lesson, you will learn:\n",
    "- How to structure prompts for clarity and reliability\n",
    "- Why formatting techniques impact the quality of model outputs\n",
    "- How prompt engineering fits into larger AI and LLM development workflows\n",
    "- How to write, refine, test, and evaluate prompts in real applications\n",
    "\n",
    "You will also write hands-on prompt examples and evaluate how formatting influences\n",
    "model responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1981f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrete_key import my_gemini_api_key\n",
    "API_KEY = my_gemini_api_key()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2893b6",
   "metadata": {},
   "source": [
    "## 1. Formatting Techniques\n",
    "Effective prompts rely on strong formatting. Here are essential tools and methods:\n",
    "\n",
    "### Placeholders\n",
    "Use placeholders similar to Python f-strings:\n",
    "```\n",
    "Explain the concept of {topic} in simple terms.\n",
    "```\n",
    "\n",
    "### Multi-line Prompts\n",
    "Combine context, rules, and tasks clearly:\n",
    "```\n",
    "You are an AI tutor.\n",
    "Task: Explain {topic}.\n",
    "Rules: Be concise.\n",
    "Format: Bullet points.\n",
    "```\n",
    "\n",
    "### Role-based Prompting\n",
    "- **System:** High-level behavior instructions\n",
    "- **User:** The user's question\n",
    "- **Assistant:** Example responses or constraints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bff31a6",
   "metadata": {},
   "source": [
    "## 2. Prompt Structuring\n",
    "A reliable prompt typically follows this structure:\n",
    "\n",
    "### Task  Context  Rules  Output Format\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Task: Summarize the document.\n",
    "Context: {document}\n",
    "Rules: Only include factual statements.\n",
    "Output Format: JSON with fields {summary, keywords}.\n",
    "```\n",
    "\n",
    "Using JSON schema improves consistency in model outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d8dd0",
   "metadata": {},
   "source": [
    "## 3. Reducing Hallucinations\n",
    "Hallucination control is critical in production systems.\n",
    "\n",
    "### Key Strategies\n",
    "- Provide grounding context\n",
    "- Use explicit, constrained instructions\n",
    "- Include few-shot examples to guide reasoning\n",
    "\n",
    "**Example Few-shot Format:**\n",
    "```\n",
    "Example Input: What is the capital of France?\n",
    "Example Output: Paris\n",
    "\n",
    "User Input: {question}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7d07f",
   "metadata": {},
   "source": [
    "## 4. Evaluation & Testing\n",
    "Prompt engineering is an iterative process.\n",
    "\n",
    "### Methods\n",
    "- Use prompt benches for A/B testing\n",
    "- Validate deterministic outputs using fixed seeds or small model variants\n",
    "- Log prompt evolution to track improvements\n",
    "\n",
    "**Goal:** Ensure prompts are stable, reproducible, and optimized for accuracy.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
