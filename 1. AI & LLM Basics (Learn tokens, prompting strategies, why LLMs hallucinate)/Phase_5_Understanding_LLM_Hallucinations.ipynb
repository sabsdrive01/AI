{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d85bcbbb",
   "metadata": {},
   "source": [
    "# üìò Phase 5: Understanding LLM Hallucinations  \n",
    "*A practical guide to identifying, understanding, and reducing hallucinations in LLMs*\n",
    "\n",
    "---\n",
    "\n",
    "## üö® **1. What Are Hallucinations?**\n",
    "\n",
    "A **hallucination** occurs when an LLM produces:\n",
    "\n",
    "- Incorrect information  \n",
    "- Fabricated facts  \n",
    "- Fake citations  \n",
    "- Overconfident‚Äîbut wrong‚Äîanswers  \n",
    "\n",
    "### üîç **Examples**\n",
    "- Claiming a person won an award they never received  \n",
    "- Inventing a research paper that doesn‚Äôt exist  \n",
    "- Giving incorrect code outputs  \n",
    "- Making up statistics  \n",
    "\n",
    "LLMs don‚Äôt ‚Äúlie.‚Äù They generate the *most probable* next tokens, which sometimes leads to confident errors.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866deb09",
   "metadata": {},
   "source": [
    "## üß† **2. Why Do LLMs Hallucinate?**\n",
    "\n",
    "Hallucinations occur for several key reasons:\n",
    "\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ Statistical Prediction ‚â† Truth**\n",
    "\n",
    "LLMs generate text based on probabilities, not factual databases.\n",
    "\n",
    "They don't \"know\"; they **predict**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Training Data Gaps**\n",
    "\n",
    "- Missing information  \n",
    "- Outdated data  \n",
    "- Biased or incorrect sources  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Ambiguous or Underspecified Prompts**\n",
    "\n",
    "If your prompt is unclear, too short, or missing constraints‚Ä¶  \n",
    "‚û°Ô∏è the model *fills in the gaps*.\n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ Over-Generalization**\n",
    "\n",
    "The model may wrongly infer patterns from examples.\n",
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ Long Context ‚Üí Memory Drift**\n",
    "\n",
    "As context windows get large, models may lose track of details.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b7649",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è **3. Mitigating Hallucinations**\n",
    "\n",
    "Here are practical strategies to reduce hallucinations in real-world systems.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **1. Grounding Responses in Context (RAG)**  \n",
    "**RAG = Retrieval-Augmented Generation**\n",
    "\n",
    "Feed the model *verified documents* to answer from.\n",
    "\n",
    "Example pipeline:\n",
    "\n",
    "1. Retrieve relevant context  \n",
    "2. Provide it to the LLM  \n",
    "3. Instruct the model to answer *only from the context*  \n",
    "\n",
    "This significantly reduces hallucination chances.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **2. Use Verifiable Sources**\n",
    "\n",
    "Tell the model explicitly:\n",
    "\n",
    "- \"Answer using the provided text only\"  \n",
    "- \"Cite sources line-by-line\"  \n",
    "- \"If unknown, respond with 'I don't know.'\"  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **3. Adjust Temperature and Top‚Äëp**\n",
    "\n",
    "High values ‚Üí more creativity ‚Üí more hallucinations  \n",
    "Low values ‚Üí more deterministic ‚Üí fewer hallucinations  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **4. Fact-Checking Loops**\n",
    "\n",
    "You can force the model to:\n",
    "\n",
    "- Reevaluate its own answer  \n",
    "- Verify claims  \n",
    "- Cross-check using a second model  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36190492",
   "metadata": {},
   "source": [
    "## üß™ **Hands-On: Simple Hallucination Test**\n",
    "\n",
    "We'll test how a model *might* hallucinate by simulating a misleading prompt.\n",
    "\n",
    "(We cannot call external APIs here, so this is a teaching demo.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8101f8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: potential hallucination scenario (simulation)\n",
    "\n",
    "def mock_model(prompt):\n",
    "    if \"who discovered ai\" in prompt.lower():\n",
    "        # Incorrect but confident output (hallucination)\n",
    "        return \"AI was discovered by Dr. Helena Carson in 1923.\"\n",
    "    return \"I don't know.\"\n",
    "\n",
    "prompt = \"Who discovered AI?\"\n",
    "mock_model(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703011c7",
   "metadata": {},
   "source": [
    "## üéØ **Summary**\n",
    "\n",
    "In this phase you learned:\n",
    "\n",
    "### ‚úî What hallucinations are  \n",
    "### ‚úî Why LLMs hallucinate  \n",
    "### ‚úî Practical mitigation strategies  \n",
    "### ‚úî RAG fundamentals  \n",
    "### ‚úî How to constrain and verify model outputs  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871d3d6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
