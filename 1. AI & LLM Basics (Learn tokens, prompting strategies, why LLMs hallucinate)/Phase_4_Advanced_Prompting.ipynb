{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0e77c3",
   "metadata": {},
   "source": [
    "# üß† Phase 4: Advanced Prompting Strategies\n",
    "\n",
    "In this phase, we explore powerful prompting patterns, reliability techniques, and common pitfalls that occur when designing prompts for large language models (LLMs).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faaabcf",
   "metadata": {},
   "source": [
    "## 1. Advanced Prompting Patterns\n",
    "\n",
    "### üîó Chain-of-Thought (CoT) Prompting\n",
    "- Encourages step-by-step reasoning.\n",
    "- Helps LLMs break down complex tasks logically.\n",
    "\n",
    "### üîÅ Self-Consistency & Voting\n",
    "- Generate multiple reasoning paths.\n",
    "- Choose the best or majority answer.\n",
    "\n",
    "### ü§ñ ReAct Prompting (Reason + Act)\n",
    "- Alternates between reasoning and performing actions.\n",
    "- Used in tool-using agents (search, calculators, code execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a97780e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Solve the problem step-by-step:\n",
    "\n",
    "Question:\n",
    "A person buys a pen for ‚Çπ20 and sells it for ‚Çπ30.\n",
    "What is the profit percentage?\n",
    "\n",
    "Think step-by-step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af69555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating multiple LLM responses (pseudo-example)\n",
    "\n",
    "responses = [\n",
    "    \"Profit = 10, Profit% = 50%\",\n",
    "    \"Profit = 10, Profit% = 33.3%\",\n",
    "    \"Profit = 10, Profit% = 50%\"\n",
    "]\n",
    "\n",
    "from collections import Counter\n",
    "Counter(responses).most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3bfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "react_prompt = \"\"\"\n",
    "You are an agent that alternates between Thought and Action steps.\n",
    "\n",
    "Thought: I need to find current weather in Chennai.\n",
    "Action: search(\"weather Chennai\")\n",
    "Observation: 31¬∞C, cloudy\n",
    "\n",
    "Thought: I can now answer the user.\n",
    "Final Answer: It is 31¬∞C and cloudy in Chennai.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad653c",
   "metadata": {},
   "source": [
    "## 2. Prompt Engineering for Reliability\n",
    "\n",
    "### üéØ Reducing Ambiguity\n",
    "- Avoid vague phrasing.\n",
    "- Specify exact task, constraints, and format.\n",
    "\n",
    "### üöß Guardrails & Constraints\n",
    "Examples:\n",
    "- ‚ÄúUse only the given data.‚Äù\n",
    "- ‚ÄúRespond in JSON format.‚Äù\n",
    "- ‚ÄúDo not make assumptions.‚Äù\n",
    "\n",
    "### üîÑ Iterative Prompt Refinement\n",
    "Process:\n",
    "1. Write prompt  \n",
    "2. Test  \n",
    "3. Analyze output  \n",
    "4. Improve prompt  \n",
    "5. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_guardrails = \"\"\"\n",
    "Summarize the text in exactly 3 bullet points.\n",
    "Do not add any new information.\n",
    "Do not exceed 10 words per bullet.\n",
    "\n",
    "Text:\n",
    "<INSERT YOUR TEXT HERE>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669a898",
   "metadata": {},
   "source": [
    "## 3. Common Prompting Pitfalls\n",
    "\n",
    "### ‚ö†Ô∏è Vague Instructions\n",
    "‚ùå \"Summarize this.\"  \n",
    "‚úÖ \"Summarize in 3 bullet points under 12 words each.\"\n",
    "\n",
    "### ‚ö†Ô∏è Overloading Context\n",
    "- Too much irrelevant data confuses the LLM.\n",
    "- Use delimiters like ```---``` or `<data></data>`.\n",
    "\n",
    "### ‚ö†Ô∏è Assuming LLMs 'Understand' Like Humans\n",
    "- Models generate patterns, not true comprehension.\n",
    "- Be explicit and structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_before = \"Explain this concept.\"\n",
    "\n",
    "prompt_after = \"\"\"\n",
    "\n",
    "Explain the concept in:\n",
    "- Simple language\n",
    "- Two paragraphs\n",
    "- With one real-world example\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97931f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
